EÄŸer dataframe bulamÄ±yorsak ve yeni Ã¶rneklerle pekiÅŸtirme yapmak istiyorsak, ÅŸu yÃ¶ntem kullanÄ±labilir; 

SKLEARN DATASETS ; 
âœ… Sklearn kÃ¼tÃ¼phanesinden alÄ±nan bu import Ã¶rnekleri bize yeni ver, setleri sunar.
âœ… -- from sklearn.datasets import load_wine -- 

Ã–RNEK ;
veri = load_wine()
df= pd.DataFrame(data = veri.data , columns=veri.feature_names)
df["proline"]= veri.target

âœ… df = pd.DataFrame(data=veri.data, columns=veri.feature_names)
Burada, yÃ¼klenen veriyi pandas DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yorsunuz:

âœ… veri.data, ÅŸarap tÃ¼rlerine dair sayÄ±sal Ã¶zelliklerin (kimyasal bileÅŸenler gibi) bulunduÄŸu numpy dizisidir.

âœ… veri.feature_names, bu sayÄ±sal Ã¶zelliklerin isimlerini (Ã¶rneÄŸin, alkol, asidite, vs.) iÃ§eren bir listedir.

âœ… df["proline"] = veri.target
Bu satÄ±rda ise ÅŸaraplarÄ±n tÃ¼rleri (veri.target) ekleniyor. veri.target, her ÅŸarabÄ±n ait olduÄŸu tÃ¼rÃ¼ belirtir (genellikle 3 farklÄ± ÅŸarap tÃ¼rÃ¼ vardÄ±r). Bu tÃ¼rler, hedef deÄŸiÅŸken olarak veri setinde bulunur. df["proline"] ifadesi, DataFrame'e yeni bir sÃ¼tun ekler ve bu sÃ¼tun ÅŸaraplarÄ±n tÃ¼r bilgilerini tutar.

---------------- STANDART SCALER -----------------------------

1. Standard Scaler Nedir?

Standard Scaler, her Ã¶zelliÄŸi (feature) ortalamasÄ± 0 ve standart sapmasÄ± 1 olacak ÅŸekilde dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

2. Neden Standardizasyon YapÄ±lÄ±r?
Makine Ã¶ÄŸrenmesi modellerinde bazÄ± algoritmalar, Ã¶zelliklerin Ã¶lÃ§eklerinden etkilenir. Standardizasyonun saÄŸladÄ±ÄŸÄ± avantajlar:

âœ… Gradient Descent HÄ±zlanÄ±r

Lineer Regresyon, Lojistik Regresyon, Yapay Sinir AÄŸlarÄ± gibi gradyan iniÅŸi (gradient descent) kullanan algoritmalar, Ã¶lÃ§eklendirilmemiÅŸ verilerle Ã§ok yavaÅŸ Ã§alÄ±ÅŸÄ±r.
âœ… Ã–zellikler EÅŸit Ã–neme Sahip Olur

EÄŸer veri setindeki bazÄ± Ã¶zelliklerin deÄŸerleri bÃ¼yÃ¼k (Ã¶rneÄŸin "gelir" = 50,000) ve bazÄ±larÄ± kÃ¼Ã§Ã¼k (Ã¶rneÄŸin "yaÅŸ" = 30) ise, bÃ¼yÃ¼k deÄŸerli deÄŸiÅŸkenler modeli domine edebilir.
Standardizasyon, tÃ¼m deÄŸiÅŸkenleri aynÄ± Ã¶lÃ§eÄŸe getirerek adil hale getirir.
âœ… Ã–klid Mesafesine DayalÄ± Algoritmalar Ä°Ã§in Gerekli

KNN (K-En YakÄ±n KomÅŸu), K-Means, PCA gibi algoritmalar Ã–klid mesafesi kullanÄ±r.
EÄŸer Ã¶zelliklerin Ã¶lÃ§ekleri farklÄ±ysa, bÃ¼yÃ¼k deÄŸerli Ã¶zellikler mesafeyi kontrol eder.
âœ… Ã‡oklu DoÄŸrusal BaÄŸlantÄ±yÄ± AzaltÄ±r

Standardizasyon, Ã§oklu doÄŸrusal baÄŸlantÄ±yÄ± (multicollinearity) azaltarak regresyon modellerinde katsayÄ±larÄ±n daha dengeli olmasÄ±nÄ± saÄŸlar.
3. Standard Scaler Hangi Durumlarda KullanÄ±lmalÄ±?
ğŸ“Œ KullanÄ±lmasÄ± gereken durumlar:

Lineer regresyon, lojistik regresyon, yapay sinir aÄŸlarÄ±, KNN, K-Means, PCA gibi Ã¶lÃ§ek duyarlÄ± algoritmalar
Veri setinde bÃ¼yÃ¼k Ã¶lÃ§ek farklarÄ± varsa
ğŸ“Œ KullanÄ±lmamasÄ± gereken durumlar:

AÄŸaÃ§ tabanlÄ± algoritmalar (Decision Tree, Random Forest, XGBoost) Ã¶lÃ§eklendirmeye duyarlÄ± deÄŸildir.

SonuÃ§;
Standard Scaler, makine Ã¶ÄŸrenmesi modellerinde veri Ã¶lÃ§eklendirmeyi saÄŸlamak, algoritmalarÄ±n daha verimli Ã§alÄ±ÅŸmasÄ±nÄ± mÃ¼mkÃ¼n kÄ±lmak ve deÄŸiÅŸkenler arasÄ±nda adil bir karÅŸÄ±laÅŸtÄ±rma yapabilmek iÃ§in kullanÄ±lÄ±r.

-- KOD BÃ–LÃœMÃœ --

# Gerekli kÃ¼tÃ¼phaneyi iÃ§e aktarÄ±yoruz
from sklearn.preprocessing import StandardScaler

# StandardScaler nesnesini oluÅŸturuyoruz
scaler = StandardScaler()

# EÄŸitim verisi Ã¼zerinde fit ve transform iÅŸlemlerini uyguluyoruz
# Bu iÅŸlem, eÄŸitim verisinin ortalamasÄ±nÄ± 0, standart sapmasÄ±nÄ± 1 yapar
X_train = scaler.fit_transform(X_train)  

# Test verisini eÄŸitim verisinde Ã¶ÄŸrenilen Ã¶lÃ§ekle dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yoruz
# Burada sadece transform kullanÄ±yoruz Ã§Ã¼nkÃ¼ fit iÅŸlemi sadece eÄŸitim setine uygulanmalÄ±
X_test = scaler.transform(X_test)

-- AÃ‡IKLAMALAR --

1ï¸âƒ£ StandardScaler() nesnesini oluÅŸturduk.

Bu nesne, veriyi ortalamasÄ± 0, standart sapmasÄ± 1 olacak ÅŸekilde Ã¶lÃ§eklendirmek iÃ§in kullanÄ±lÄ±r.
2ï¸âƒ£ fit_transform(X_train) ile eÄŸitim verisini Ã¶lÃ§eklendirdik.

fit(X_train): OrtalamayÄ± ve standart sapmayÄ± hesaplar.
transform(X_train): X_trainâ€™i Ã¶lÃ§eklendirir.
fit_transform(X_train): Fit ve transform iÅŸlemlerini tek adÄ±mda yapar.
3ï¸âƒ£ transform(X_test) ile test verisini dÃ¶nÃ¼ÅŸtÃ¼rdÃ¼k.

Test verisinin ortalamasÄ±nÄ± deÄŸiÅŸtirmek istemiyoruz.
EÄŸitim verisinden Ã¶ÄŸrenilen ortalama ve standart sapmayÄ± kullanarak aynÄ± Ã¶lÃ§eÄŸe getiriyoruz.

----------------------------------------------------------------------
----------------- Ridge Regresyon (L2 Regularization) ----------------

Ridge regresyon, Ã§oklu doÄŸrusal baÄŸlantÄ± (multicollinearity) ve overfitting sorunlarÄ±nÄ± Ã§Ã¶zmek iÃ§in kullanÄ±lÄ±r. Lineer regresyona L2 regularizasyonu ekleyerek katsayÄ±larÄ± kÃ¼Ã§Ã¼ltÃ¼r ama tam sÄ±fÄ±ra indirmez.

ğŸ“Œ RidgeCV, en iyi regularization (Î±) deÄŸerini otomatik bulmamÄ±zÄ± saÄŸlar.
ğŸ“Œ Model, overfittingâ€™i Ã¶nler ve daha saÄŸlam hale gelir.
ğŸ“Œ RMSE, MAE ve R^2
  metrikleri ile modelin performansÄ±nÄ± deÄŸerlendiririz.

1. Ridge Regresyonun KullanÄ±m AmaÃ§larÄ±
âœ… Ã‡oklu DoÄŸrusal BaÄŸlantÄ±yÄ± (Multicollinearity) Azaltmak

EÄŸer veri setinde baÄŸÄ±msÄ±z deÄŸiÅŸkenler (features) birbirleriyle Ã§ok iliÅŸkiliyse (multicollinearity), normal lineer regresyon kararsÄ±z hale gelir.
Ridge regresyon, katsayÄ±larÄ± kÃ¼Ã§Ã¼lterek modelin daha saÄŸlam (robust) olmasÄ±nÄ± saÄŸlar.

âœ… Overfittingâ€™i Engellemek

Normal lineer regresyon, eÄŸitim verisine Ã§ok fazla uyum saÄŸlayarak test verisinde kÃ¶tÃ¼ sonuÃ§ verebilir (overfitting).
Ridge regresyon, katsayÄ±larÄ± sÄ±nÄ±rlandÄ±rarak modelin genelleÅŸtirme gÃ¼cÃ¼nÃ¼ artÄ±rÄ±r.

âœ… YÃ¼ksek Boyutlu (High-Dimensional) Verilerde Daha Ä°yi Performans

Ã–zellikle Ã§ok fazla Ã¶zellik (feature) iÃ§eren veri setlerinde, Ridge regresyon aÅŸÄ±rÄ± uÃ§ deÄŸerlere karÅŸÄ± daha dayanÄ±klÄ±dÄ±r.
Ã–rneÄŸin, biyoenformatik, finans, gÃ¶rÃ¼ntÃ¼ iÅŸleme gibi alanlarda Ã§ok sayÄ±da deÄŸiÅŸken iÃ§eren veri setlerinde kullanÄ±lÄ±r.

ğŸ‘‰ EÄŸer tÃ¼m deÄŸiÅŸkenleri kullanmak istiyorsan Ridge, bazÄ±larÄ±nÄ± tamamen sÄ±fÄ±rlayÄ±p gereksiz olanlarÄ± elemek istiyorsan Lasso kullan!
ğŸ‘‰ EÄŸer hem Ridge hem Lassoâ€™nun avantajlarÄ±nÄ± istiyorsan Elastic Net kullan!

-- KOD BÃ–LÃœMÃœ --

# Ridge Regresyon'u iÃ§e aktarÄ±yoruz
from sklearn.linear_model import Ridge

# Ridge modelini oluÅŸturuyoruz ve alpha (regularization parametresi) deÄŸerini belirliyoruz
ridge_model = Ridge(alpha=1)

# Modeli eÄŸitim verisi (X_train, y_train) ile eÄŸitiyoruz
ridge_model.fit(X_train, y_train)

# Test verisi ile tahmin yapÄ±yoruz
ridge_pred = ridge_model.predict(X_test)

# Performans Ã¶lÃ§Ã¼mleri iÃ§in gerekli metrikleri iÃ§e aktarÄ±yoruz
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# R^2 (Determinasyon KatsayÄ±sÄ±): Modelin veriyle ne kadar iyi uyum saÄŸladÄ±ÄŸÄ±nÄ± gÃ¶sterir (1'e yakÄ±n olmasÄ± iyidir)
print("R^2 : ", round(r2_score(y_test, ridge_pred), 2))

# MAE (Mean Absolute Error): GerÃ§ek ve tahmin deÄŸerleri arasÄ±ndaki mutlak farkÄ±n ortalamasÄ± (daha kÃ¼Ã§Ã¼k olmasÄ± iyidir)
print("MAE : ", round(mean_absolute_error(y_test, ridge_pred), 2))

# RMSE (Root Mean Squared Error): Hata karelerinin ortalamasÄ±nÄ±n karekÃ¶kÃ¼ (daha kÃ¼Ã§Ã¼k olmasÄ± iyidir)
print("RMSE : ", round(mean_squared_error(y_test, ridge_pred, squared=False), 2))


-- Kodun AdÄ±m AdÄ±m AÃ§Ä±klamasÄ± --
âœ… 1. Ridge Regresyon Modeli OluÅŸturuluyor:
Ridge(alpha=1):

alpha=1, Ridge regresyonunun regularizasyon parametresidir.
KÃ¼Ã§Ã¼k alpha (0'a yakÄ±n) â†’ Daha fazla esneklik â†’ Overfitting riski artar.
BÃ¼yÃ¼k alpha â†’ Daha fazla katsayÄ± kÃ¼Ã§Ã¼ltme â†’ Underfitting olabilir.
âœ… 2. Model EÄŸitiliyor:
ridge_model.fit(X_train, y_train)

Model, X_train iÃ§indeki Ã¶zellikleri ve y_train iÃ§indeki hedef deÄŸiÅŸkeni kullanarak eÄŸitilir.
âœ… 3. Test Seti Ãœzerinde Tahmin YapÄ±lÄ±yor:
ridge_pred = ridge_model.predict(X_test)

EÄŸitim sonrasÄ± model, X_test verisi Ã¼zerinde tahmin yapar.
âœ… 4. Modelin BaÅŸarÄ±mÄ± Ã–lÃ§Ã¼lÃ¼yor:
ğ‘…^2
  (R-Kare) â†’ Modelin veriye ne kadar iyi uyduÄŸunu gÃ¶sterir (1â€™e yakÄ±n olmalÄ±).
MAE (Ortalama Mutlak Hata) â†’ GerÃ§ek ve tahmin edilen deÄŸerler arasÄ±ndaki mutlak farklarÄ± Ã¶lÃ§er (kÃ¼Ã§Ã¼k olmalÄ±).
RMSE (KÃ¶k Ortalama Kare Hata) â†’ HatalarÄ±n karekÃ¶kÃ¼ alÄ±narak Ã¶lÃ§Ã¼lÃ¼r (kÃ¼Ã§Ã¼k olmalÄ±).


-- EN Ä°YÄ° ALPHA DEÄERÄ°NÄ° BULMAK Ä°Ã‡Ä°N YAPILMASI GEREKENLER --

-- Kodun AdÄ±m AdÄ±m AÃ§Ä±klamasÄ±--
âœ… 1. RidgeCV ile En Ä°yi Alpha DeÄŸerini SeÃ§iyoruz

RidgeCV(alphas=[0.1, 1, 10, 100, 1000])
Cross-validation kullanarak en iyi alpha (regularization) deÄŸerini otomatik seÃ§er.
En iyi alpha deÄŸeri ridge_cv.alpha_ ile bulunur.
âœ… 2. En Ä°yi Alpha DeÄŸeriyle Ridge Modeli EÄŸitiyoruz

Ridge(alpha=best_alpha) ile Ridge modelini en uygun regularization parametresiyle kuruyoruz.
âœ… 3. Test Seti Ãœzerinde Tahmin YapÄ±yoruz

ridge_pred1 = ridge_model1.predict(X_test)
âœ… 4. Modelin PerformansÄ±nÄ± Ã–lÃ§Ã¼yoruz
R^2â†’ Modelin veriyle uyumunu gÃ¶sterir (1â€™e yakÄ±n olmasÄ± iyi).
MAE (Ortalama Mutlak Hata) â†’ HatalarÄ±n mutlak deÄŸerlerinin ortalamasÄ± (kÃ¼Ã§Ã¼k olmasÄ± iyi).
RMSE (KÃ¶k Ortalama Kare Hata) â†’ HatalarÄ±n karesinin ortalamasÄ±nÄ±n karekÃ¶kÃ¼ (kÃ¼Ã§Ã¼k olmasÄ± iyi).


-- KOD BÃ–LÃœMÃœ --

# Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktarÄ±yoruz
from sklearn.linear_model import Ridge, RidgeCV
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# Alpha (Î») deÄŸerini en iyi seÃ§mek iÃ§in RidgeCV kullanÄ±yoruz
# Belirli alpha deÄŸerleri arasÄ±ndan en iyisini bulur

ridge_cv = RidgeCV(alphas=[0.1, 1, 10, 100, 1000], store_cv_values=True)

# store_cv_values=True, RidgeCV modelini kullanÄ±rken Ã§apraz doÄŸrulama (cross-validation) deÄŸerlerinin saklanmasÄ±nÄ± saÄŸlar. Bu, modelin performansÄ±nÄ± deÄŸerlendirmek iÃ§in kullanÄ±lan Ã§apraz doÄŸrulama sÃ¼recinin ayrÄ±ntÄ±larÄ±nÄ± saklamak amacÄ±yla kullanÄ±lÄ±r.


# Modeli eÄŸitim verisiyle eÄŸitiyoruz
ridge_cv.fit(X_train, y_train)

# En iyi alpha deÄŸerini buluyoruz
best_alpha = ridge_cv.alpha_
print("En iyi alpha deÄŸeri:", best_alpha)

# En iyi alpha deÄŸeriyle Ridge modelini oluÅŸturuyoruz
ridge_model1 = Ridge(alpha=best_alpha)

# Modeli eÄŸitiyoruz
ridge_model1.fit(X_train, y_train)

# Test seti Ã¼zerinde tahmin yapÄ±yoruz
ridge_pred1 = ridge_model1.predict(X_test)

# Performans Ã¶lÃ§Ã¼mlerini yazdÄ±rÄ±yoruz
print("R^2 : ", round(r2_score(y_test, ridge_pred1), 2))
print("MAE : ", round(mean_absolute_error(y_test, ridge_pred1), 2))
print("RMSE : ", round(mean_squared_error(y_test, ridge_pred1, squared=False), 2))

r^2 = 0.85
mae = 0.26 = 1 alpha iÃ§in
rmse = 0.1

R^2 :  0.85
MAE :  0.25 = 10 alpha iÃ§in
RMSE :  0.1

R^2 :  0.8
MAE :  0.3 = 100 alpha iÃ§in
RMSE :  0.13

-----------------------------------------------------------------------------------
------------------------------ LASSO ---------------------------------------

ğŸ“Œ Lasso Neden KullanÄ±lÄ±r?
Lasso (Least Absolute Shrinkage and Selection Operator) regresyonu, Ã¶zellik seÃ§imi (feature selection) ve model sadeleÅŸtirme amacÄ±yla kullanÄ±lÄ±r.

âœ… Lasso KullanÄ±m AmaÃ§larÄ±

1ï¸âƒ£ Gereksiz DeÄŸiÅŸkenleri (Ã–zellikleri) Eleme (Feature Selection)
Lasso, bazÄ± katsayÄ±larÄ± sÄ±fÄ±ra Ã§ekerek gereksiz Ã¶zellikleri tamamen Ã§Ä±karÄ±r.
EÄŸer veri setinde Ã§ok fazla gereksiz deÄŸiÅŸken varsa, Lasso Ã¶nemli olanlarÄ± bÄ±rakÄ±p diÄŸerlerini sÄ±fÄ±r yapar.
ğŸ“Œ Ã–rnek:
EÄŸer bir veri setinde 100 deÄŸiÅŸken varsa ve 30'u gereksizse, Lasso bunlarÄ± sÄ±fÄ±ra indirerek modeli sadeleÅŸtirir.

2ï¸âƒ£ Daha Basit ve Yorumlanabilir Modeller Ãœretme
Modelin daha az deÄŸiÅŸkenle Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar, bÃ¶ylece yorumlamasÄ± ve analiz edilmesi kolay olur.
Ã‡ok fazla deÄŸiÅŸken iÃ§eren bir model yerine, sadece en Ã¶nemli deÄŸiÅŸkenleri iÃ§eren bir model daha iyi olabilir.

--- KOD BÃ–LÃœMÃœ ---

# ğŸ“Œ Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktarÄ±yoruz
from sklearn.linear_model import LassoCV
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# ğŸ“Œ LassoCV modelini oluÅŸturuyoruz
lasso_cv = LassoCV(
    eps=0.1,        # Alpha deÄŸerleri arasÄ±nda minimum ve maksimum oranÄ±nÄ± belirler.
                    # (min_alpha = eps * max_alpha)
    n_alphas=100,   # Alpha iÃ§in 100 farklÄ± deÄŸer belirlenir ve Ã§apraz doÄŸrulama ile en iyisi seÃ§ilir.
    cv=5            # 5 katlÄ± Ã§apraz doÄŸrulama (Veriyi 5 parÃ§aya bÃ¶ler, her parÃ§a iÃ§in modeli test eder)
)

# ğŸ“Œ Modeli eÄŸitim verisi ile eÄŸitiyoruz
lasso_cv.fit(X_train, y_train)

# ğŸ“Œ SeÃ§ilen en iyi Alpha deÄŸerini yazdÄ±rÄ±yoruz
print("SeÃ§ilen en iyi alpha deÄŸeri:", lasso_cv.alpha_)

# ğŸ“Œ Modelin katsayÄ±larÄ±nÄ± yazdÄ±rÄ±yoruz
print("Modelin katsayÄ±larÄ± (coef_):", lasso_cv.coef_)

# ğŸ“Œ Lasso bazÄ± katsayÄ±larÄ± sÄ±fÄ±ra indirebilir (Gereksiz gÃ¶rdÃ¼ÄŸÃ¼ deÄŸiÅŸkenleri tamamen kaldÄ±rÄ±r)
# EÄŸer bazÄ± katsayÄ±lar 0 ise, Lasso o deÄŸiÅŸkenleri Ã¶nemli gÃ¶rmediÄŸini belirtmiÅŸ olur.

# ğŸ“Œ Test verisi ile tahmin yapÄ±yoruz
lasso_pred = lasso_cv.predict(X_test)

# ğŸ“Œ Modelin performansÄ±nÄ± deÄŸerlendiriyoruz
print("R^2 : ", round(r2_score(y_test, lasso_pred), 2))  # Modelin aÃ§Ä±klayÄ±cÄ±lÄ±k oranÄ±
print("MAE : ", round(mean_absolute_error(y_test, lasso_pred), 2))  # Ortalama mutlak hata
print("RMSE : ", round(mean_squared_error(y_test, lasso_pred, squared=False), 2))  # KÃ¶k ortalama kare hata

--- KODUN AÃ‡IKLAMASI ---

ğŸ“Œ Kodun AÃ§Ä±klamasÄ±
LassoCV Modeli TanÄ±mlandÄ±

-- eps=0.1 â†’ Alpha deÄŸerleri arasÄ±nda minimum ve maksimum oranÄ±nÄ± belirler.

Artarsa: Daha geniÅŸ bir alpha aralÄ±ÄŸÄ± seÃ§ilir ve daha fazla potansiyel alpha deÄŸeri denenir.
AzalÄ±rsa: alpha deÄŸerleri daha dar bir aralÄ±kta kalÄ±r, bu da modelin cezalandÄ±rma seviyelerini daha sÄ±nÄ±rlÄ± tutar.

-- n_alphas=100 â†’ 100 farklÄ± ceza katsayÄ±sÄ± (alpha) denenecek.

Artarsa: Daha fazla alpha deÄŸeri denenir, yani daha ince bir seÃ§im yapÄ±lÄ±r. Bu, daha doÄŸru sonuÃ§lar verebilir ama aynÄ± zamanda iÅŸlem sÃ¼resini uzatabilir.
AzalÄ±rsa: Daha az alpha deÄŸeri denenir, iÅŸlem sÃ¼resi daha kÄ±sa olur ancak modelin doÄŸru sonucu bulma olasÄ±lÄ±ÄŸÄ± azalabilir.

cv=5 â†’ 5 katlÄ± Ã§apraz doÄŸrulama ile en iyi alpha deÄŸeri belirlenecek.

Bu parametre, Ã§apraz doÄŸrulama sÄ±rasÄ±nda veri kÃ¼mesini kaÃ§ katman (fold) halinde bÃ¶leceÄŸinizi belirtir. 5 katlÄ± Ã§apraz doÄŸrulama, veri kÃ¼mesini 5 parÃ§aya bÃ¶ler ve her birini sÄ±rasÄ±yla test seti olarak kullanÄ±r, kalan 4 parÃ§ayÄ± ise eÄŸitim seti olarak kullanÄ±r.
Artarsa: Daha fazla katmanlÄ± Ã§apraz doÄŸrulama yapÄ±lÄ±r, modelin daha genellenebilir olma olasÄ±lÄ±ÄŸÄ± artar ancak iÅŸlem sÃ¼resi de uzar.
AzalÄ±rsa: Ã‡apraz doÄŸrulama daha hÄ±zlÄ± gerÃ§ekleÅŸir, fakat modelin genellenebilirliÄŸi dÃ¼ÅŸebilir.


Model EÄŸitildi (fit ile)

lasso_cv.alpha_ â†’ En iyi bulunan alpha deÄŸeri alÄ±ndÄ±.
lasso_cv.coef_ â†’ Modelin belirlediÄŸi katsayÄ±lar alÄ±ndÄ±. BazÄ± katsayÄ±lar sÄ±fÄ±r olabilir!
Tahmin YapÄ±ldÄ± (predict ile)

lasso_pred = lasso_cv.predict(X_test) â†’ Model test verisi iÃ§in tahmin yaptÄ±.
Modelin PerformansÄ± Ã–lÃ§Ã¼ldÃ¼

RÂ² â†’ Modelin aÃ§Ä±klayÄ±cÄ±lÄ±k oranÄ± (Ne kadar iyi tahmin yapÄ±yor?)
MAE â†’ Ortalama mutlak hata (GerÃ§ek deÄŸerlerle tahminler arasÄ±ndaki farkÄ±n ortalamasÄ±)
RMSE â†’ KÃ¶k ortalama kare hata (Tahminlerin doÄŸruluÄŸunu Ã¶lÃ§mek iÃ§in daha duyarlÄ± bir hata metriÄŸi)


ğŸš€ Ã–zet
âœ” Lasso, bazÄ± deÄŸiÅŸkenlerin katsayÄ±larÄ±nÄ± sÄ±fÄ±ra Ã§ekerek gereksiz deÄŸiÅŸkenleri ortadan kaldÄ±rÄ±r.
âœ” Ã–zellik seÃ§imi yaparak daha basit ve yorumlanabilir modeller oluÅŸturur.
âœ” AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi azaltabilir, genelleÅŸtirilebilir modeller oluÅŸturabilir.
âœ” Ã–zellikle yÃ¼ksek boyutlu veri setlerinde gereksiz Ã¶zellikleri eleyerek daha iyi performans saÄŸlar.

ğŸ“Œ EÄŸer tÃ¼m katsayÄ±lar sÄ±fÄ±r deÄŸilse, Ridge regresyonu kullanmak daha mantÄ±klÄ± olabilir.
ğŸ“Œ EÄŸer sadece en Ã¶nemli deÄŸiÅŸkenleri seÃ§mek istiyorsak, Lasso daha iyi bir seÃ§enek olur.
-----------------------------------------------------------------------------------
-------------------------- ELASTÄ°C-NET ---------------------------------------

from sklearn.linear_model import ElasticNetCV

elastic_model = ElasticNetCV(l1_ratio= (0.01 , 0.1, 0.2 , 0.3,0.4 ,0.5 , 0.6 , 0.7 , 0.8 , 0.9 , 0.95 , 0.99 , 1) , tol=0.01)


1ï¸âƒ£ l1_ratio Nedir?
ElasticNet, hem Lasso (L1 cezasÄ±) hem de Ridge (L2 cezasÄ±) regresyonlarÄ±nÄ± birleÅŸtiren bir yÃ¶ntemdir.

ğŸ”¹ l1_ratio, L1 (Lasso) ile L2 (Ridge) arasÄ±ndaki dengeyi ayarlar.

â„¹ï¸ l1_ratio Ä°Ã§in DeÄŸerler ve AnlamlarÄ±

l1_ratio		Model TÃ¼rÃ¼
0		Sadece Ridge (L2 cezasÄ±) uygulanÄ±r.
1		Sadece Lasso (L1 cezasÄ±) uygulanÄ±r.
0.5		L1 ve L2 cezasÄ±nÄ±n eÅŸit karÄ±ÅŸÄ±mÄ± (ElasticNet)
0 < l1_ratio < 1	L1 ve L2'nin karÄ±ÅŸÄ±mÄ± (ElasticNet)
ğŸ“Œ Kodundaki l1_ratio parametresi bir dizi (tuple) olarak verilmiÅŸ.

(0.01, 0.1, 0.2, ..., 1) deÄŸerleri denenerek en iyi oran (l1_ratio) seÃ§ilir.
Bu sayede, model farklÄ± L1-L2 kombinasyonlarÄ±nÄ± test ederek en iyi dengeyi belirler.

2ï¸âƒ£ tol (Tolerans) Nedir?
ğŸ”¹ tol, optimizasyon sÃ¼recinin hassasiyetini (erken durdurma toleransÄ±nÄ±) belirler.

KÃ¼Ã§Ã¼k tol deÄŸerleri â†’ Daha hassas ama daha yavaÅŸ hesaplama.
BÃ¼yÃ¼k tol deÄŸerleri â†’ Daha hÄ±zlÄ± ama daha az hassas sonuÃ§.
â„¹ï¸ Genellikle tol=0.0001 gibi kÃ¼Ã§Ã¼k deÄŸerler tercih edilir. Ancak kodundaki tol=0.01, optimizasyon sÃ¼recini biraz hÄ±zlandÄ±racaktÄ±r.

ğŸ”¥ Ã–zet
âœ” l1_ratio, L1 (Lasso) ve L2 (Ridge) regresyonlarÄ± arasÄ±ndaki dengeyi ayarlar.
âœ” tol, optimizasyon sÃ¼recinin hassasiyetini belirleyerek erken durdurma toleransÄ±nÄ± kontrol eder.

--- ğŸ”¥ KOD BÃ–LÃœMÃœ ğŸ”¥ ---

# Gerekli kÃ¼tÃ¼phaneyi iÃ§e aktarÄ±yoruz
from sklearn.linear_model import ElasticNetCV
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# ElasticNetCV modelini oluÅŸturuyoruz
elastic_model = ElasticNetCV(
    l1_ratio=(0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 1),  # L1 ve L2 dengesini belirleyecek deÄŸerler
    tol=0.01  # Optimizasyon hassasiyeti (erken durdurma toleransÄ±)
)

# Modeli eÄŸitim verisi ile eÄŸitiyoruz
elastic_model.fit(X_train, y_train)

# SeÃ§ilen en iyi L1 oranÄ±nÄ± alÄ±yoruz
print("En iyi L1 oranÄ± (l1_ratio_):", elastic_model.l1_ratio_)

# Modelin tahmin iÃ§in kullanacaÄŸÄ± katsayÄ±larÄ± alÄ±yoruz
print("Model katsayÄ±larÄ± (coef_):", elastic_model.coef_)

# EÄŸer coef_'lerin hiÃ§biri sÄ±fÄ±r deÄŸilse, Ridge regresyonun aÄŸÄ±rlÄ±klÄ± olarak seÃ§ildiÄŸini anlÄ±yoruz.
# L1 cezasÄ± (Lasso) bazÄ± katsayÄ±larÄ± sÄ±fÄ±ra Ã§ektiÄŸi iÃ§in, sÄ±fÄ±r olmayan tÃ¼m katsayÄ±lar Ridge etkisini gÃ¶sterir.

# Test verisi Ã¼zerinde tahmin yapÄ±yoruz
elastic_pred = elastic_model.predict(X_test)

# R^2 skorunu hesaplÄ±yoruz (modelin aÃ§Ä±klayÄ±cÄ±lÄ±k oranÄ±)
print("R^2 : ", round(r2_score(y_test, elastic_pred), 2))

# Ortalama mutlak hata (MAE) hesaplanÄ±yor
print("MAE : ", round(mean_absolute_error(y_test, elastic_pred), 2))

# KÃ¶k ortalama kare hata (RMSE) hesaplanÄ±yor
print("RMSE : ", round(mean_squared_error(y_test, elastic_pred, squared=False), 2))  # squared=False RMSE hesaplar


ğŸš€ Kodun Ne YapÄ±yor?
Bu kod, farklÄ± L1-L2 oranlarÄ±nÄ± (l1_ratio) test ederek en iyi ElasticNet modelini bulmaya Ã§alÄ±ÅŸÄ±yor. Tolerans 0.01 olarak ayarlandÄ±ÄŸÄ± iÃ§in, Ã§ok kÃ¼Ã§Ã¼k deÄŸiÅŸikliklerde model eÄŸitimi durabilir, yani model biraz daha hÄ±zlÄ± Ã§alÄ±ÅŸacaktÄ±r.






