-------------------- POLÄ°NOM REGRESYON  --------------------

Polinom regresyon, doÄŸrusal regresyonun genelleÅŸtirilmiÅŸ bir versiyonudur ve doÄŸrusal olmayan iliÅŸkileri modellemek iÃ§in kullanÄ±lÄ±r. Bu yÃ¶ntemde baÄŸÄ±mlÄ± deÄŸiÅŸken (Ã§Ä±ktÄ±) ile baÄŸÄ±msÄ±z deÄŸiÅŸken (girdi) arasÄ±ndaki iliÅŸki, bir polinom denklemi ile ifade edilir.

Basit doÄŸrusal regresyon ÅŸu ÅŸekilde yazÄ±lÄ±r:
y = bâ‚€x^2 + bâ‚x + c

Burada bâ‚€ ve bâ‚ katsayÄ±lar, c hata terimidir.

Polinom regresyonda ise baÄŸÄ±msÄ±z deÄŸiÅŸkenin Ã¼st dereceli terimleri eklenerek denklem geniÅŸletilir:
y = bâ‚€ + bâ‚x + bâ‚‚xÂ² + bâ‚ƒxÂ³ + ... + bâ‚™xâ¿ + Îµ

Burada n, polinomun derecesidir.

Polinom regresyon ne zaman kullanÄ±lÄ±r?

EÄŸer veriler doÄŸrusal bir modelle iyi temsil edilemiyorsa, polinom regresyon daha uygun olabilir.
Verilerde belirli bir eÄŸilim olup olmadÄ±ÄŸÄ±nÄ± anlamak iÃ§in kullanÄ±lÄ±r.

AvantajlarÄ±:

DoÄŸrusal regresyonun yetersiz kaldÄ±ÄŸÄ± durumlarda daha iyi modelleme yapabilir.
KarmaÅŸÄ±k veri setleri iÃ§in daha esnek bir model sunar.

DezavantajlarÄ±:

Derece Ã§ok yÃ¼kselirse aÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting) riski oluÅŸabilir.
YÃ¼ksek dereceli polinomlar hesaplama aÃ§Ä±sÄ±ndan maliyetli olabilir.
Python'da polinom regresyon, sklearn.preprocessing.PolynomialFeatures kÃ¼tÃ¼phanesi ile uygulanabilir. Modelin doÄŸruluÄŸunu artÄ±rmak iÃ§in uygun derece seÃ§ilmeli ve model deÄŸerlendirme metrikleri kullanÄ±lmalÄ±dÄ±r.

-------------------- POLÄ°NOM REGRESYON ADIMLARI  --------------------

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt    == import
import seaborn as sns
 
---------------------------------------------------------------------                

df = pd.read_csv("community_health_evolved.csv")
x= df.drop("visits" , axis=1) #  X deÄŸiÅŸkeni (baÄŸÄ±msÄ±z deÄŸiÅŸkenler) 
y= df["visits"]

== Bu kod, Makine Ã–ÄŸrenmesi (ML) veya Veri Analizi projelerinde kullanÄ±lmak Ã¼zere baÄŸÄ±msÄ±z (X) ve baÄŸÄ±mlÄ± (y) deÄŸiÅŸkenleri ayÄ±rmak iÃ§in yazÄ±lmÄ±ÅŸtÄ±r.

EÄŸer regresyon modeli (Ã¶rneÄŸin, doÄŸrusal regresyon veya polinom regresyon) oluÅŸturmak istiyorsan, X ve y'yi modelin eÄŸitimi iÃ§in kullanabilirsin

---------------------------------------------------------------------

from sklearn.preprocessing import PolynomialFeatures

# PolynomialFeatures, baÄŸÄ±msÄ±z deÄŸiÅŸkenlere polinom terimleri eklemek iÃ§in kullanÄ±lan bir sklearn sÄ±nÄ±fÄ±dÄ±r.

# Polinom dÃ¶nÃ¼ÅŸÃ¼mÃ¼nÃ¼ tanÄ±mla (derece 2, bias terimi eklenmez)
ply_conv = PolynomialFeatures(degree=2, include_bias=False)

# Parametreler:
# degree=2 â†’ Ä°kinci dereceden polinom terimleri ekler.
# include_bias=False â†’ Bilinmeyen (intercept) terimi eklenmez.

# Veri boyutunu kontrol et
x.shape  # x'in (satÄ±r sayÄ±sÄ±, sÃ¼tun sayÄ±sÄ±) ÅŸeklinde boyutunu dÃ¶ndÃ¼rÃ¼r.
# Burada normalde (1000,6) lÄ±k bir deÄŸer Ã§Ä±kar.
# fit_transform(x) de detaylÄ± anlatÄ±m saÄŸlanmÄ±ÅŸtÄ±r.

# Polinom dÃ¶nÃ¼ÅŸÃ¼mÃ¼nÃ¼ uygula
ply_feat = ply_conv.fit_transform(x)

fit_transform(x):
# - x iÃ§indeki baÄŸÄ±msÄ±z deÄŸiÅŸkenleri ikinci dereceden polinom terimlerine geniÅŸletir.
# - EÄŸer x baÅŸlangÄ±Ã§ta 6 sÃ¼tuna sahipse, dÃ¶nÃ¼ÅŸÃ¼mden sonra 27 sÃ¼tuna Ã§Ä±kabilir.
# - Bunun formÃ¼lÃ¼ ((n) *(n+1)) / 2 den gelir. 
# - 6*7 /2 = 21
# - BaÅŸtaki 6 da Ã¼stÃ¼ne eklenir ve toplam 27 deÄŸer olur. 

# Alternatif kullanÄ±m:
# ply_conv.fit(x)  # Modeli veriye uydurur (sadece Ã¶ÄŸrenme yapar).
# ply_conv.transform(x)  # Daha Ã¶nce Ã¶ÄŸrenilen dÃ¶nÃ¼ÅŸÃ¼mÃ¼ uygular.
# fit_transform(x) â†’ Hem Ã¶ÄŸrenir hem de dÃ¶nÃ¼ÅŸÃ¼mÃ¼ uygular.

---------------------------------------------------------------------

-------------------- TRAÄ°N TEST SPLÄ°T  --------------------

from sklearn.model_selection import train_test_split

# Veriyi eÄŸitim ve test setlerine ayÄ±r
# test_size=0.3 â†’ Verinin %30'u test, %70'i eÄŸitim iÃ§in kullanÄ±lÄ±r
# random_state=99 â†’ SonuÃ§larÄ±n tekrar Ã¼retilebilir olmasÄ± iÃ§in sabit rastgelelik kullanÄ±lÄ±r
X_train, x_test, y_train, y_test = train_test_split(ply_feat, y, test_size=0.3, random_state=99)

from sklearn.linear_model import LinearRegression

# Lineer regresyon modelini tanÄ±mla
# fit_intercept=True â†’ Modelin sabit terimi (bias) Ã¶ÄŸrenmesini saÄŸlar
model = LinearRegression(fit_intercept=True)

# Modeli eÄŸitim verisiyle eÄŸit
model.fit(X_train, y_train)

# Test verisi Ã¼zerinde tahmin yap
poly_pred = model.predict(x_test)

# Tahmin edilen deÄŸerleri yazdÄ±r
poly_pred

---------------------------------------------------------------------

----------------------- PERFORMANS ---------------------------

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import seaborn as sns

# Mean Absolute Error (MAE) hesapla
# MAE â†’ GerÃ§ek ve tahmin edilen deÄŸerler arasÄ±ndaki ortalama mutlak farkÄ± Ã¶lÃ§er
mae = mean_absolute_error(y_test, poly_pred)

# Root Mean Squared Error (RMSE) hesapla
# RMSE â†’ Hata karelerinin ortalamasÄ±nÄ±n karekÃ¶kÃ¼nÃ¼ alarak bÃ¼yÃ¼k hatalara daha fazla aÄŸÄ±rlÄ±k verir
rmse = np.sqrt(mean_squared_error(y_test, poly_pred))

# R^2 Skoru (R-kare) hesapla
# R^2 â†’ Modelin baÄŸÄ±msÄ±z deÄŸiÅŸkenleri ne kadar iyi aÃ§Ä±kladÄ±ÄŸÄ±nÄ± gÃ¶sterir (1'e yakÄ±n olmasÄ± iyidir)
r2 = r2_score(y_test, poly_pred)

# SonuÃ§larÄ± ekrana yazdÄ±r
print("MAE :", mae)
print("RMSE :", rmse)
print("R2 :", r2)

# Veri setinin Ã§ift deÄŸiÅŸkenli iliÅŸkilerini gÃ¶rselleÅŸtir
sns.pairplot(df)

---------------------------------------------------------------------

?????????????????????????????????????????????????????????????????????

----------------------- NP.POLYFÄ°T() ---------------------------

Polinom regresyonu ile bir veri kÃ¼mesine en iyi uyan polinomu belirler.
Verilen x ve y deÄŸerleri iÃ§in en uygun n. dereceden polinom katsayÄ±larÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.
Veriler doÄŸrusal olmayan bir yapÄ±ya sahipse, yÃ¼ksek dereceli polinomlar daha iyi modelleme yapabilir.

x= df["TV"]
y= df["sales"]

# 0 dan 300 e 100 adet sayÄ±
harcamalar = np.linspace(0,300 ,100) 

--- 1. DERECE FONKSÄ°YONLAR Ä°Ã‡Ä°N ( ax +b ) ---

# x ve y verilerini kullanarak 1. dereceden polinom (lineer) uydurur.
np.polyfit(x,y,1) 

satis = 0.04753664 * harcamalar + 7.03259355

--- 2. DERECE FONKSÄ°YONLAR Ä°Ã‡Ä°N ( ax^2 + bx + c ) ---

# x ve y verilerini kullanarak 2. dereceden polinom (lineer) uydurur.
np.polyfit(x,y,2)

satis = -6.84693373e-05 * harcamalar**2 +  6.72659270e-02*harcamalar +  6.11412013e+00

sns.scatterplot(data=df , x="TV" ,  y = "sales" )
plt.plot(harcamalar ,satis , color="r" , lw=4)

--- 3. DERECE FONKSÄ°YONLAR Ä°Ã‡Ä°N ( ax^3 + bx^2 + cx +d ) ---

# x ve y verilerini kullanarak 3. dereceden polinom (lineer) uydurur.
np.polyfit(x,y,3)

satis = 5.57199796e-07 * harcamalar **3 + -3.15222433e-04*harcamalar**2 + 9.64341770e-02*harcamalar+ 5.42010655e+00

sns.scatterplot(data=df , x="TV" ,  y = "sales" )
plt.plot(harcamalar ,satis , color="r" , lw=4)


# BÃ¼tÃ¼n bu iÅŸlemler sonucunda modelin yapÄ±sÄ±nÄ± anladÄ±k.
# Åimdi denediÄŸimiz bu polinom regresyon modelinden Ã§Ä±karÄ±mlar yapalÄ±m.
# Bir sonraki adÄ±m train test split

----------------------- TRAÄ°N TEST SPLÄ°T ---------------------------

x = df.drop("sales" , axis=1)
y= df["sales"]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    x, y, test_size=0.3, random_state=99)

?????????????????????????????????????????????????????????????????????

# Ã–NCE LÄ°NEER MODEL DE KONTROL EDELÄ°M...

--- ?? NEDEN POLY. REGR UYGULARKEN LÄ°NN REGR. KULLANIYORUZ ?? ---

Ã‡Ã¼nkÃ¼ polinom regresyonun formu katsayÄ±lar aÃ§Ä±sÄ±ndan hala lineerdir.

from sklearn.linear_model import LinearRegression

model_linear = LinearRegression()
model_linear.fit(X_train,y_train)

pred_lin = model_linear.predict(X_test)

?????????????????????????????????????????????????????????????????????

# PERFORMANSLARINI Ä°NCELEYELÄ°M...

from sklearn.metrics import r2_score , mean_absolute_error , mean_squared_error

mae = mean_absolute_error(y_test , pred_lin)
rmse = np.sqrt(mean_squared_error(y_test , pred_lin))

print("Mae  : " , mae)
print("Rmse :", rmse)

res_lin = y_test - pred_lin

sns.scatterplot(x=y_test , y = res_lin)

sns.histplot(res_lin,bins=30 , kde=True)

# FÄ°NAL BÄ°R LÄ°NEER REGRESYON OLUÅTURUP POLY Ä°LE FARKINI Ä°NCELEYELÄ°M...

final_lin = LinearRegression()
final_lin.fit(x,y)

y_hat = final_lin.predict(x)

----------------- GRAFÄ°K -----------------
fig, ax = plt.subplots(1,3,figsize=(15,5))

ax[0].plot(df["TV"], df["sales"], "o", color="blue")
ax[0].plot(df["TV"], y_hat, "o", color="red")
ax[0].set_ylabel("Sales", size=14)
ax[0].set_title("TV")

ax[1].plot(df["radio"], df["sales"], "o", color="blue")
ax[1].plot(df["radio"], y_hat, "o", color="red")
ax[1].set_title("RADYO")

ax[2].plot(df["newspaper"], df["sales"], "o", color="blue")
ax[2].plot(df["newspaper"], y_hat, "o", color="red")
ax[2].set_title("GAZETE")

plt.tight_layout()

# Mavi gerÃ§ek
# KÄ±rmÄ±zÄ± bizim Ã§Ä±kardÄ±ÄŸÄ±mÄ±z.
---------------------------------------------

# POLÄ°NOM MODELE GEÃ‡Ä°Å YAPALIM...

from sklearn.preprocessing import PolynomialFeatures

poly_conv = PolynomialFeatures(degree=2 , include_bias=False)
poly_feat = poly_conv.fit_transform(x)

poly_feat.shape # # 3 * 4 /2

# poly_feat, polinom Ã¶zelliklerini iÃ§erdiÄŸi iÃ§in, bu Ã¶zelliklerle yapÄ±lan eÄŸitim ve test bÃ¶lÃ¼nmesi, modelin doÄŸrusal olmayan iliÅŸkiyi Ã¶ÄŸrenmesine olanak tanÄ±r. EÄŸer sadece temel x Ã¶zellikleri kullanÄ±lmÄ±ÅŸ olsaydÄ±, model doÄŸrusal bir iliÅŸkiyi Ã¶ÄŸrenmiÅŸ olurdu, fakat burada amaÃ§ daha karmaÅŸÄ±k bir model kurmaktÄ±r.

X_train, X_test, y_train, y_test = train_test_split(
    poly_feat, y, test_size=0.3, random_state=99)


model_poly = LinearRegression(fit_intercept=True)

model_poly.fit(X_train , y_train)


pred_polinom = model_poly.predict(X_test)

# pred_polinom deÄŸiÅŸkeni, modelin polinom regresyonu ile yapÄ±lan tahmin sonuÃ§lar demektir.
mae = mean_absolute_error(y_test , pred_polinom)
rmse = np.sqrt(mean_squared_error(y_test , pred_polinom))
r2= r2_score(y_test, pred_polinom)


print("Mae  : ", mae)
print("Rmse : ", rmse)
print("R2   : ",r2)


*******************************************************************************************

------------------------ EN Ä°YÄ° POLÄ°NOM DERECESÄ° ------------------------

# RMSE (Root Mean Squared Error) deÄŸerlerini saklamak iÃ§in boÅŸ listeler oluÅŸturuyoruz
train_rmse = []
test_rmse = []

# Polinom derecesini 1'den 9'a kadar deÄŸiÅŸtirerek test ediyoruz
for d in range(1, 10):
    
    # Belirtilen dereceye sahip polinom Ã¶zelliklerini oluÅŸtur
    poly_conv = PolynomialFeatures(degree=d, include_bias=False)
    poly_feat = poly_conv.fit_transform(x)  # x verisini polinom Ã¶zelliklerine dÃ¶nÃ¼ÅŸtÃ¼r
    
    # Veriyi eÄŸitim ve test setlerine ayÄ±r (test seti %30, eÄŸitim seti %70)
    X_train, X_test, y_train, y_test = train_test_split(poly_feat, y, test_size=0.3, random_state=99)
    
    # Lineer Regresyon modelini oluÅŸtur ve eÄŸit
    model = LinearRegression(fit_intercept=True)
    model.fit(X_train, y_train)
    
    # Modelin eÄŸitim ve test setleri Ã¼zerinde tahmin yapmasÄ±nÄ± saÄŸla
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    
    # EÄŸitim ve test setleri iÃ§in RMSE hesapla
    train_RMSE = np.sqrt(mean_squared_error(y_train, train_pred))
    test_RMSE = np.sqrt(mean_squared_error(y_test, test_pred))

    # Hesaplanan RMSE deÄŸerlerini listelere ekle
    train_rmse.append(train_RMSE)
    test_rmse.append(test_RMSE)

# train_rmse ve test_rmse listeleri, farklÄ± polinom dereceleri iÃ§in hata deÄŸerlerini iÃ§erir


ÅÄ°MDÄ° SIRA GÃ–RSELLEÅTÄ°RME DE : 

plt.plot(range(1,7),train_rmse[:6],label='TRAIN')
plt.plot(range(1,7),test_rmse[:6],label='TEST')
plt.xlabel("Polinom Derecesi")
plt.ylabel("RMSE")
plt.legend()

-- Burada Ã§Ä±kan gÃ¶rselde 4 deÄŸerinde uÃ§uk bir artÄ±ÅŸ vardÄ±r
-- Bu uÃ§uk deÄŸer bize polinom derecesinin 4 olduÄŸunu sÃ¶yler.

Ä°yi Modeli SeÃ§mek Ä°Ã§in ;

-- Train RMSE ve test RMSE yakÄ±n olmalÄ± ve test RMSE en dÃ¼ÅŸÃ¼k olduÄŸu dereceyi seÃ§melisin.
-- En iyi model test hatasÄ±nÄ±n en dÃ¼ÅŸÃ¼k olduÄŸu noktada bulunur.


================== DEÄERÄ° 4 BULDUKTAN SONRA ============================

# 4. dereceden polinom Ã¶zellikleri oluÅŸtur
poly_reg = PolynomialFeatures(degree=4)
x_poly = poly_reg.fit_transform(x)  # x verisini polinom dÃ¶nÃ¼ÅŸÃ¼mÃ¼ne tabi tut

# Polinom regresyon modeli oluÅŸtur ve eÄŸit
poly_reg_final = LinearRegression()
poly_reg_final.fit(x_poly, y)  # Modeli eÄŸit

# ArtÄ±k poly_reg_final modelini kullanarak tahmin yapabilirsin
y_pred = poly_reg_final.predict(x_poly)

poly_reg_pred = poly_reg_final.predict(x_Poly)

'''
poly_reg_final modeli, daha Ã¶nce eÄŸittiÄŸimiz polinom regresyon modelidir.
predict(x_poly) fonksiyonu, polinom dÃ¶nÃ¼ÅŸÃ¼mÃ¼ yapÄ±lmÄ±ÅŸ x_poly verisini kullanarak tahmin yapar.
SonuÃ§ olarak, modelin tahmin ettiÄŸi y deÄŸerlerini poly_reg_pred iÃ§ine kaydederiz.
'''
# HATALARI BULALIM ;
mae = mean_absolute_error(y , poly_reg_pred)
rmse = np.sqrt(mean_squared_error(y , poly_reg_pred))
r2= r2_score(y, poly_reg_pred)
print("Mae  : ", mae)
print("Rmse : ", rmse)
print("R2   : ",r2)


# MODELÄ°N NE KADAR Ä°YÄ° Ã‡ALIÅTIÄINA BAKALIM

sns.scatterplot(x=x["TV"] ,y= y , color ="red" , data=df)
plt.scatter(x["TV"] , poly_reg_pred , color="blue",s=10


sns.scatterplot(x=x["radio"] ,y= y , color ="red" , data=df)
plt.scatter(x["radio"] , poly_reg_pred , color="blue",s=10)


sns.scatterplot(x=X["TV"], y=y, color = 'red',data=df) # ["TV"] olmadan Ã§alÄ±ÅŸtÄ±r
plt.scatter(X ,poly_pred_3, color = 'blue', s=10)
plt.title('Truth or Bluff (Polynomial Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

Bu adÄ±mlarÄ±n sonunda en iyi polinom derecesi ile model eÄŸitilmiÅŸ ve yukarÄ±da anlatÄ±mlarÄ± saÄŸlanmÄ±ÅŸtÄ±r.

*******************************************************************************************

				ğŸš€ Polinom Regresyon ile DeÄŸer Tahmini NasÄ±l YapÄ±lÄ±r?

ğŸ”¹ 1ï¸âƒ£ Polinom DÃ¶nÃ¼ÅŸÃ¼mÃ¼ Uygula

 Ã–ncelikle, eÄŸitim aÅŸamasÄ±nda kullandÄ±ÄŸÄ±mÄ±z PolynomialFeatures dÃ¶nÃ¼ÅŸÃ¼mÃ¼nÃ¼, tahmin yapacaÄŸÄ±mÄ±z veri iÃ§in de uygulamalÄ±yÄ±z. Ã‡Ã¼nkÃ¼ modelimiz, polinom formunda bir veri bekliyor! ğŸ¯

deger_poly = poly_conv.transform(np.array([44.5, 39.3, 45.1]).reshape(1, -1))

ğŸ“Œ Burada ne yaptÄ±k?
âœ… np.array([44.5, 39.3, 45.1]): Tahmin etmek istediÄŸimiz giriÅŸ verisi.
âœ… .reshape(1, -1): 2D matris formatÄ±na Ã§evirdik (Ã‡Ã¼nkÃ¼ scikit-learn bunu bekliyor).
âœ… poly_conv.transform(...): Modeli eÄŸitirken kullandÄ±ÄŸÄ±mÄ±z PolynomialFeatures dÃ¶nÃ¼ÅŸÃ¼mÃ¼nÃ¼ bu yeni giriÅŸ verisine uyguladÄ±k.

ğŸ”¹ 2ï¸âƒ£ Model ile Tahmin Yap
 Polinom dÃ¶nÃ¼ÅŸÃ¼mÃ¼nÃ¼ uyguladÄ±ktan sonra, artÄ±k modelimizi kullanarak tahmin yapabiliriz! ğŸš€

tahmin = model.predict(deger_poly)
print(tahmin)

ğŸ“Œ Burada ne yaptÄ±k?
âœ… model.predict(deger_poly): Ã–ÄŸrenilmiÅŸ model ile tahmini hesapladÄ±k.
âœ… print(tahmin): Sonucu ekrana yazdÄ±rdÄ±k.

âš  Ã–nemli Noktalar

ğŸ”¸ poly_conv, eÄŸitim aÅŸamasÄ±nda kullandÄ±ÄŸÄ±mÄ±z PolynomialFeatures nesnesi olmalÄ±dÄ±r. EÄŸer kaybettiysen, yeniden oluÅŸturman gerekir!
ğŸ”¸ reshape(1, -1) neden gerekli?
Ã‡Ã¼nkÃ¼ scikit-learn, giriÅŸ verisini her zaman 2D bir matris olarak bekler. EÄŸer 1D bir dizi verirsen hata alÄ±rsÄ±n! ğŸš¨

ğŸ¯ Ã–zetle:
ğŸ“Œ Ã–nce tahmin yapmak istediÄŸimiz veriyi polinom formuna dÃ¶nÃ¼ÅŸtÃ¼rdÃ¼k.
ğŸ“Œ Sonra modeli kullanarak tahmini hesapladÄ±k.

âœ¨ Ve iÅŸte sonuÃ§! ğŸš€ğŸ”®
-------------------------------------------------------------------------------------------

# include_bias = True

- Polinom hale gelmiÅŸ veri setine tamamÄ± birlerden oluÅŸan bir sÃ¼tÃ¼n ekler

- include_bias=True, genellikle baÄŸÄ±msÄ±z deÄŸiÅŸkenler kÃ¼mesinde sabit terimi iÃ§ermeyen durumlarda kullanÄ±lÄ±r.

EÄŸer include_bias=False olarak ayarlanÄ±rsa, sabit terim (bias) eklenmez ve modelin orijinden geÃ§mesi saÄŸlanÄ±r.

-------------------------------------------------------------------------------------------


