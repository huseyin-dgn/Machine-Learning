LÄ°NEER REGRESYON NEDÄ°R ?

* Lineer Regresyon'un temel amacÄ±, baÄŸÄ±msÄ±z deÄŸiÅŸkenler (features) ile baÄŸÄ±mlÄ± deÄŸiÅŸken (target) arasÄ±ndaki doÄŸrusal iliÅŸkiyi modellemek ve bu iliÅŸkiyi kullanarak tahmin yapmaktÄ±r.

Temel Hedef:-

* BaÄŸÄ±msÄ±z deÄŸiÅŸkenler (Ã¶rneÄŸin, yaÅŸ, kilo, boy gibi faktÃ¶rler) ile baÄŸÄ±mlÄ± deÄŸiÅŸken (Ã¶rneÄŸin, bir kiÅŸinin saÄŸlÄ±k durumu, ziyaret sayÄ±sÄ±, fiyat gibi hedef deÄŸiÅŸken) arasÄ±ndaki iliÅŸkileri anlamak ve bu iliÅŸkiyi bir doÄŸrusal denklemle modellemek.

	      ----------- Lineer Regresyon Ä°ÅŸlemleri --------------

1ï¸âƒ£ Veri Setinin Ä°ncelenmesi ve Temizlenmesi
2ï¸âƒ£ Ã–zellikler (X) ve Hedef (y) Belirlenmesi
3ï¸âƒ£ EÄŸitim ve Test Verilerinin AyrÄ±lmasÄ± (Train-Test Split)
4ï¸âƒ£ Modelin OluÅŸturulmasÄ± (LinearRegression Nesnesi OluÅŸturma)
5ï¸âƒ£ Modelin EÄŸitilmesi (fit Fonksiyonu ile EÄŸitim)
6ï¸âƒ£ Modelin Tahmin YapmasÄ± (predict Fonksiyonu ile)
7ï¸âƒ£ Model PerformansÄ±nÄ±n DeÄŸerlendirilmesi
8ï¸âƒ£ Modelin Kaydedilmesi ve YÃ¼klenmesi (Joblib ile)


1-) KÃ¼tÃ¼phaneleri import ediyoruz.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn  as sns
%matplotlib inline # EÄŸer grafikler AÃ§Ä±lmazsa

-------------------------------------------------------------------------

2-) Dataframe i belirtiyoruz

df = pd.read_csv("community_health.csv")

"df" yazarak dataframe i inceleyebilirsiniz.

-------------------------------------------------------------------------

			Veri Setinin Ä°ncelenmesi ve Temizlenmesi
		     (df.info(), df.describe(), sns.pairplot(), vb.)

df.info() = Bir pandas DataFrame'inin yapÄ±sÄ±nÄ± Ã¶zetleyen bilgileri gÃ¶sterir. Bu komut, veri kÃ¼mesindeki her bir sÃ¼tun hakkÄ±nda genel bir bakÄ±ÅŸ saÄŸlar.

df.describe() =  count: SÃ¼tundaki eksik olmayan (non-null) deÄŸer sayÄ±sÄ±.
		 mean: SÃ¼tundaki ortalama (aritmetik ortalama) deÄŸeri.
		 std: SÃ¼tundaki deÄŸerlerin standart sapmasÄ±
		 min: SÃ¼tundaki en kÃ¼Ã§Ã¼k (minimum) deÄŸer.
		 25%: SÃ¼tundaki 1. Ã§eyrek deÄŸeri, yani %25'lik dilim.
		 50%: SÃ¼tundaki medyan yani %50'lik dilim.
		 75%: SÃ¼tundaki 3. Ã§eyrek deÄŸeri, yani %75'lik dilim.
		 max: SÃ¼tundaki en bÃ¼yÃ¼k (maximum) deÄŸer.

df.isnull() = BoÅŸ mu dolu mu olduÄŸu kontrol edilir.

df.isnull().sum() = BoÅŸ olan kolonlarÄ±n sayÄ±sal verilerini verir.


* corr fonksiyonu * : 

 -- df["age"].corr(df["visits"]) 
 ==  "age" (yaÅŸ) sÃ¼tunu ile "visits" (ziyaret sayÄ±sÄ±) sÃ¼tunu arasÄ±ndaki Pearson korelasyon katsayÄ±sÄ±nÄ± hesaplar.

 -- Korelasyon Nedir -- 

Korelasyon, iki deÄŸiÅŸken arasÄ±ndaki iliÅŸkinin doÄŸrusal olup olmadÄ±ÄŸÄ±nÄ± ve bu iliÅŸkinin pozitif mi, negatif mi yoksa sÄ±fÄ±r mÄ± olduÄŸunu belirler.

Pozitif Korelasyon: Bir deÄŸiÅŸken arttÄ±kÃ§a diÄŸer deÄŸiÅŸken de artar.
Negatif Korelasyon: Bir deÄŸiÅŸken arttÄ±kÃ§a diÄŸer deÄŸiÅŸken azalÄ±r.
SÄ±fÄ±r Korelasyon: Ä°ki deÄŸiÅŸken arasÄ±nda doÄŸrusal bir iliÅŸki yoktur.
Korelasyon deÄŸeri genellikle -1 ile 1 arasÄ±nda bir deÄŸer alÄ±r:

1: MÃ¼kemmel pozitif doÄŸrusal iliÅŸki (biri arttÄ±kÃ§a diÄŸeri de artar).
-1: MÃ¼kemmel negatif doÄŸrusal iliÅŸki (biri arttÄ±kÃ§a diÄŸeri azalÄ±r).
0: HiÃ§bir doÄŸrusal iliÅŸki yoktur (rastgele daÄŸÄ±lÄ±r).

-- Korelasyon KullanÄ±m AmacÄ± --

** Ä°ki DeÄŸiÅŸken ArasÄ±ndaki Ä°liÅŸkiyi Anlamak **

Bu komut, yaÅŸ (age) ile ziyaret sayÄ±sÄ± (visits) arasÄ±ndaki doÄŸrusal iliÅŸkiyi anlamanÄ±zÄ± saÄŸlar. Ã–rneÄŸin, yaÅŸ arttÄ±kÃ§a ziyaret sayÄ±sÄ±nÄ±n artÄ±p artmadÄ±ÄŸÄ±nÄ± ya da azalÄ±p azalmadÄ±ÄŸÄ±nÄ± Ã¶ÄŸrenirsiniz.

* Pairplot ile *

sns.pairplot(df) = Pairplot, veri setindeki her bir sayÄ±sal sÃ¼tun iÃ§in, diÄŸer tÃ¼m sayÄ±sal sÃ¼tunlarla olan iliÅŸkileri gÃ¶steren scatter plot'lar (daÄŸÄ±lÄ±m grafikleri) oluÅŸturur. AyrÄ±ca, diagonal (Ã§apraz) kÄ±sÄ±mlarda, her bir sayÄ±sal deÄŸiÅŸkenin kendi histogramÄ±nÄ± veya kernel density plot (yoÄŸunluk tahmini) grafiÄŸini gÃ¶sterir.

-------------------------------------------------------------------------
				   Train Test Split

			Ã–zellikler (X) ve Hedef (y) Belirlenmesi
	(Ä°lgili sÃ¼tunlarÄ±n seÃ§ilmesi, Ã¶rn: x = df.drop("visits", axis=1), y = df["visits"])

" sns.pairplot(df , diag_kind="kde") ":

	diag_kind="kde" parametresi, grafiklerin diagonal (Ã§apraz) alanlarÄ±nda Kernel Density Estimate (KDE) grafiÄŸi Ã§izmeyi belirtir.
Diagonal alanlarda daha Ã¶nce histogramlar gÃ¶rÃ¼nÃ¼rken, kde seÃ§eneÄŸi ile yoÄŸunluk tahmin grafiÄŸi (Kernel Density Plot) Ã§izilir. Bu, verinin sÃ¼rekli daÄŸÄ±lÄ±mÄ±nÄ± daha pÃ¼rÃ¼zsÃ¼z bir ÅŸekilde gÃ¶rselleÅŸtirir.

	Hist yazarsak da farklÄ± bir gÃ¶rselleÅŸtirme Ã§Ä±kar.Histin amacÄ± sayÄ±sal analizi doÄŸrudan gÃ¶zlemlemektir

'''
df.rename(
    columns={"age " : "yas",
             "gender": "cinsiyet",
             "race/ethnicity": "irk",
             "weight":"kilo",
             "height" : "boy"
             }, inplace=True
    )

 Kolon isimlerini deÄŸiÅŸtirdik.

'''

df.head(3) = Dataframe deki ilk 3 deÄŸere baktÄ±k.


df["kilo"] = round(df["kilo"] *0.45) = Kg ye dÃ¶nÃ¼ÅŸtÃ¼rdÃ¼k

df["boy"] = round(df["boy"] * 2.54) = Cm ye dÃ¶nÃ¼ÅŸtÃ¼rdÃ¼k

df["cinsiyet"] = df["cinsiyet"].replace({"female": 0, "male": 1})
# Female : 0
# Male : 1

df["irk"] = df["irk"].replace({"group A" :1 ,"group B" : 2 , "group C" : 3 , "group D" :4 ,"group E" : 5}.)


df.to_csv("community_health_MLLR.csv" , index=False) = DÃ¼zeltilen dataframe i kayÄ±t ettik.

# index=False, CSV dosyasÄ±na DataFrame'in indeks sÃ¼tununun yazÄ±lmamasÄ±nÄ±

-------------------------------------------------------------------------
		EÄŸitim ve Test Verilerinin AyrÄ±lmasÄ± (Train-Test Split)
			(train_test_split ile verinin bÃ¶lÃ¼nmesi)

 Train-Test Split, makine Ã¶ÄŸrenmesi ve veri bilimi projelerinde kullanÄ±lan bir tekniktir ve modelin genelleme yeteneÄŸini test etmek iÃ§in oldukÃ§a Ã¶nemlidir. AmaÃ§, modelin eÄŸitim (training) ve test (testing) aÅŸamalarÄ±ndaki performansÄ±nÄ± deÄŸerlendirebilmektir.

x= df.drop("visits" , axis=1) == X i visits kolunu olmayan bir hale getir 
y=df["visits"] == Y yi de yalnÄ±zca visits olacak ÅŸekilde deÄŸerlendir.               

# AXÄ°S = 1 => SÃœTÃœN
# AXÄ°S = 0 => SATIR


	" from sklearn.model_selection import train_test_split " 


		X_train, X_test, y_train, y_test = train_test_split(
   			x, y, test_size=0.3, random_state=99)

# eÄŸitim seti = Train : Model, bu veriden Ã¶ÄŸrenir ve kalÄ±plarÄ± keÅŸfeder
# test seti = Test : EÄŸitim sÄ±rasÄ±nda hiÃ§ gÃ¶rmediÄŸi verilerle test edilir.
# test_size =Buu test setinin oranÄ±nÄ± ifade eder.Ã–rneÄŸin, test_size=0.33 ifadesi, verinin %33'Ã¼nÃ¼n test seti, geri kalan %67'sinin eÄŸitim seti olarak ayrÄ±lacaÄŸÄ±nÄ± belirtir.

			" X_train.head() "

Bu komut, X_train verisindeki ilk 5 satÄ±rÄ± gÃ¶sterir. Burada, her bir satÄ±r bir eÄŸitim Ã¶rneÄŸini ve her bir sÃ¼tun ise Ã¶zellikleri (input variables) temsil eder.
-------------------------------------------------------------------------
		Modelin OluÅŸturulmasÄ± (LinearRegression Nesnesi OluÅŸturma)

	"" from sklearn.linear_model import LinearRegression # import ""

regr = LinearRegression() : Regr adÄ±nda regresyon Ã¶rneÄŸi
			    LinearRegression sÄ±nÄ±fÄ±ndan bir nesne oluÅŸturuluyor.
			    'regr', doÄŸrusal regresyon modelini temsil eden bir Ã¶rnek olacak.

-------------------------------------------------------------
		Modelin EÄŸitilmesi (fit Fonksiyonu ile EÄŸitim)


			regr.fit(X_train , y_train)

'''
regr: Ã–nceki adÄ±mdaki gibi, doÄŸrusal regresyon modelini temsil eden nesne.

X_train: Modelin eÄŸitim verisi olan Ã¶zellikler (input features). Bu, modelin tahmin yapabilmesi iÃ§in gerekli olan veriler.

y_train: EÄŸitim verisinin hedef deÄŸiÅŸkeni veya etiketleri (output labels). Model, bu verileri kullanarak doÄŸru tahminleri Ã¶ÄŸrenmeye Ã§alÄ±ÅŸacaktÄ±r.

'''
-------------------------------------------------------------
		test_preds = regr.predict(X_test)

regr: Ã–nceki adÄ±mda eÄŸitilen doÄŸrusal regresyon modelini temsil eder.

X_test: Test verisi, yani modelin eÄŸitim sÄ±rasÄ±nda hiÃ§ gÃ¶rmediÄŸi veriler. Bu veriler, modelin genelleme yeteneÄŸini test etmek iÃ§in kullanÄ±lÄ±r.

test_preds: Bu deÄŸiÅŸken, modelin X_test Ã¼zerinde yaptÄ±ÄŸÄ± tahminlerin (predictions) sonuÃ§larÄ±nÄ± iÃ§erir.

test_preds deÄŸerini direkt yazdÄ±rdÄ±ÄŸÄ±mÄ±zda her deÄŸer iÃ§in gerekli tahminlerin sayÄ±sÄ±na eriÅŸebiliriz.
-------------------------------------------------------------

	from sklearn.metrics import r2_score, mean_absolute_error,mean_squared_error 

Scikit-learn kÃ¼tÃ¼phanesinden Ã¼Ã§ farklÄ± regresyon baÅŸarÄ± metriÄŸini import eder. Bu metrikler, modelin tahmin performansÄ±nÄ± deÄŸerlendirmek iÃ§in kullanÄ±lÄ±r.

r2_score: BÃ¼yÃ¼k bir RÂ² deÄŸeri daha iyi demek
	
    TanÄ±m: RÂ² (R-kare), modelin baÄŸÄ±mlÄ± deÄŸiÅŸkendeki (hedef deÄŸiÅŸken) varyansÄ± ne kadar aÃ§Ä±klayabildiÄŸini Ã¶lÃ§en bir istatistiksel deÄŸerdir. YÃ¼ksek bir RÂ² deÄŸeri, modelin doÄŸru tahminler yapma yeteneÄŸini gÃ¶sterir.

mean_absolute_error (MAE): KÃ¼Ã§Ã¼k bir MAE deÄŸeri daha iyi demektir

   TanÄ±m: Mean Absolute Error (MAE), modelin tahminlerinin ortalama mutlak hatasÄ±nÄ± Ã¶lÃ§er. Yani, her bir tahminin ne kadar "yanlÄ±ÅŸ" olduÄŸunu gÃ¶sterir. DÃ¼ÅŸÃ¼k bir MAE deÄŸeri, modelin doÄŸru tahminler yaptÄ±ÄŸÄ± anlamÄ±na gelir.
Hesaplanma: TÃ¼m tahmin hatalarÄ±nÄ±n mutlak deÄŸeri alÄ±nÄ±r ve bunlarÄ±n ortalamasÄ± alÄ±nÄ±r.

mean_squared_error (MSE): KÃ¼Ã§Ã¼k bir MSE deÄŸeri daha iyi demektir
   TanÄ±m: Mean Squared Error (MSE), modelin tahminlerinin kareli hata ortalamasÄ±dÄ±r. MSE, hatalarÄ±n bÃ¼yÃ¼klÃ¼ÄŸÃ¼ne duyarlÄ±dÄ±r ve bÃ¼yÃ¼k hatalarÄ± daha fazla cezalandÄ±rÄ±r. Bu metrik, Ã¶zellikle daha bÃ¼yÃ¼k hatalarÄ±n cezalandÄ±rÄ±lmasÄ±nÄ± istediÄŸinizde kullanÄ±lÄ±r.
Hesaplanma: HatalarÄ±n karesi alÄ±nÄ±r ve ardÄ±ndan bunlarÄ±n ortalamasÄ± hesaplanÄ±r.

- Åimdi bu deÄŸerleri nasÄ±l yazdÄ±racaÄŸÄ±mÄ±za ve yorumlayacaÄŸÄ±mÄ±za bakalÄ±m -

 	--	print("Theta :     ", (regr.coef_[0]))     --

regr.coef_[0]: Bu, modelin eÄŸim katsayÄ±sÄ± (Î¸1) deÄŸeridir.
EÄŸim katsayÄ±sÄ±, regresyon denkleminde baÄŸÄ±msÄ±z deÄŸiÅŸkenin (input) hedef deÄŸiÅŸken Ã¼zerindeki etkisini gÃ¶sterir. 
Yani, bu sayÄ±, baÄŸÄ±msÄ±z deÄŸiÅŸkenin her bir birim artÄ±ÅŸÄ±nÄ±n, hedef deÄŸiÅŸkende (yani "visits" gibi) ne kadar bir deÄŸiÅŸim yaratacaÄŸÄ±nÄ± ifade eder.
Ã–rneÄŸin, diyelim ki modelin Î¸1 deÄŸeri 0.5. Bu durumda, her bir 1 birim artÄ±ÅŸta, hedef deÄŸiÅŸkende (Ã¶rneÄŸin, "visits") 0.5 birim artÄ±ÅŸ olacaÄŸÄ± anlamÄ±na gelir.


          --     print("Intercept : ", (regr.intercept_))      -- 

Intercept, regresyon denklemindeki sabit terimdir ve baÄŸÄ±msÄ±z deÄŸiÅŸkenin deÄŸeri sÄ±fÄ±r olduÄŸunda hedef deÄŸiÅŸkenin (output) aldÄ±ÄŸÄ± deÄŸeri gÃ¶sterir.
Yani, baÄŸÄ±msÄ±z deÄŸiÅŸkenin (Ã¶rneÄŸin "age" ya da "weight") deÄŸeri sÄ±fÄ±r olduÄŸunda, hedef deÄŸiÅŸkenin deÄŸeri bu sabit terim (intercept) olacaktÄ±r.


     --    print("R^2 : ", round(r2_score(y_test,test_preds),2))    --

RÂ² (R kare) istatistiÄŸi, modelin baÄŸÄ±msÄ±z deÄŸiÅŸkenler ile baÄŸÄ±mlÄ± deÄŸiÅŸken arasÄ±ndaki iliÅŸkiyi ne kadar iyi aÃ§Ä±kladÄ±ÄŸÄ±nÄ± Ã¶lÃ§er. Yani, modelin hedef deÄŸiÅŸkenin varyansÄ±nÄ± ne kadar aÃ§Ä±kladÄ±ÄŸÄ±nÄ± belirtir.

# 2 : OndalÄ±klÄ± sayÄ±ya yuvarla demektir.

RÂ² deÄŸeri 1'e yaklaÅŸtÄ±kÃ§a, modelin gerÃ§ek verilerle uyumu artar. RÂ² = 0 olduÄŸunda, model hiÃ§bir anlamlÄ± bilgi saÄŸlamaz, sadece verilerin ortalamasÄ±nÄ± kullanÄ±yor gibi dÃ¼ÅŸÃ¼nÃ¼lebilir.
RÂ² deÄŸeri negatif bile olabilir, bu da modelin ortalama tahmin yapmaktan bile kÃ¶tÃ¼ olduÄŸu anlamÄ±na gelir.


--   print("MAE :", round(mean_absolute_error(y_test,test_preds),2))   --

MAE, modelin yaptÄ±ÄŸÄ± tahminlerin ne kadar yanÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶sterir. Tahminlerin her birinin ne kadar yanlÄ±ÅŸ olduÄŸunu Ã¶lÃ§er ve ardÄ±ndan bunlarÄ±n ortalamasÄ±nÄ± alÄ±r.
KÃ¼Ã§Ã¼k bir MAE deÄŸeri, modelin doÄŸru tahminler yaptÄ±ÄŸÄ± anlamÄ±na gelir. EÄŸer MAE 0 ise, model tamamen doÄŸru tahminler yapÄ±yor demektir.


print("RMSE :", round(np.sqrt(mean_squared_error(y_test,test_preds)),2))


MSE, tahminlerin ne kadar yanlÄ±ÅŸ olduÄŸunu Ã¶lÃ§en bir baÅŸka metriktir, ancak bu sefer hata karelerinin ortalamasÄ± alÄ±nÄ±r. Bu, bÃ¼yÃ¼k hatalarÄ± daha fazla cezalandÄ±rÄ±r.
MSE'nin karekÃ¶kÃ¼ alÄ±narak RMSE (Root Mean Squared Error) hesaplanÄ±r. RMSE, hata miktarÄ±nÄ± gerÃ§ek birimler cinsinden gÃ¶sterir (Ã¶rneÄŸin, ziyaret sayÄ±sÄ± gibi).
RMSE deÄŸeri kÃ¼Ã§Ã¼k olduÄŸunda, modelin doÄŸru tahminler yaptÄ±ÄŸÄ± sÃ¶ylenebilir. EÄŸer RMSE 0 ise, model hatasÄ±z Ã§alÄ±ÅŸÄ±yor demektir.

-------------------------------------------------------------

	--      	   Final         	 -- 


final = LinearRegression()

final.fit(x,y) 

# x modelin tahmin yapabilmesi iÃ§in gerekli girdi verileri

# y ise modelin tahmin etmeye Ã§alÄ±ÅŸtÄ±ÄŸÄ± Ã§Ä±ktÄ± verisidir.

# fit fonksiyonu, doÄŸrusal regresyon modelini veriye gÃ¶re eÄŸitmek iÃ§in kullanÄ±lÄ±r.

y_hat = final.predict(x)

#  Bu, modelin daha Ã¶nce Ã¶ÄŸrendiÄŸi doÄŸrusal iliÅŸkiyi kullanarak, verilen x (baÄŸÄ±msÄ±z deÄŸiÅŸkenler) iÃ§in tahminler yapmasÄ±nÄ± saÄŸlar.

final.coef_ 

# BurasÄ± bize her kolonun hesaplamada ne kadar etkisi olduÄŸunu gÃ¶sterir.

coef_df = pd.DataFrame(final.coef_ , x.columns , columns=["KatsayÄ±"])

# Bu ÅŸekilde yazarak coef_df deÄŸerini Ã§aÄŸÄ±rÄ±rsak tablolanmÄ±ÅŸ biÃ§imde gÃ¶stermiÅŸ oluruz.

print(df["age"].mean())
print(df["cinsiyet"].mean())
print(df["irk"].mean())
print(df["kilo"].mean())
print(df["boy"].mean())

# Her kolondaki ortalama deÄŸeri hesapladÄ±k.
-------------------------------------------------------------

		Modelin Tahmin YapmasÄ± (predict Fonksiyonu ile)

insan_evladÄ± =[[30 , 0 , 1 , 60, 164 ]]
# Koyulan 2 parantez 2 boyutlu dizi olduÄŸunu belirtir

final.predict(insan_evladÄ±)
# Makine deÄŸer tahmin etti:

x.iloc[135]
# 135. index deÄŸeri getir demek istiyor

denek= [[45,1,4,107,180]] 
# 135. deÄŸerdeki kolonlarÄ± yeni bir veri gibi yazdÄ±rdÄ±k.

Bu incelemeler sonucunda modelin ne kadar doÄŸru Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± anlayabiliriz.

-------------------------------------------------------------

	Modelin Kaydedilmesi ve YÃ¼klenmesi (Joblib ile)

from joblib import dump , load

dump(final, "dr_hastsa_visit.joblib")

model_down= load("dr_hastsa_visit.joblib")
# model_down artÄ±k, kaydedilen modelin bir kopyasÄ±dÄ±r ve tahmin yapabilmek iÃ§in kullanÄ±labilir.

model_down.predict(hyso)

-------------------------------------------------------------

	--      	Residual ( Hata Terimi ) 	  --

ğŸ“Œ Residual (Hata Terimi), gerÃ§ek deÄŸer ile tahmin edilen deÄŸer arasÄ±ndaki farktÄ±r.

ğŸ“Œ Matematiksel olarak:

    Resudial = GerÃ§ek DeÄŸer - Tahmin edilen DeÄŸer


resudial= y_hat -y 
# Her veri iÃ§in hata tahmini 

plt.figure(figsize=(12,8), dpi = 120)
sns.scatterplot(x=y , y=residual , s=70)
plt.axhline(y=0 , color="r", ls="-" , lw=4)

# Bu ÅŸekilde de gÃ¶rselleÅŸtirilebilir.
 

  		     -- HATA TAHMÄ°NÄ°NÄ° Ä°NCELEYELÄ°M --

# 2 satÄ±r ve 3 sÃ¼tundan oluÅŸan bir grid (subplot) oluÅŸturuyoruz, her bir grafiÄŸi bu eksenlere yerleÅŸtireceÄŸiz
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 12))

# Ä°lk eksen: Age vs Visits (GerÃ§ek veri ve tahminler)
axes[0, 0].plot(df['age'], df['visits'], 'o')  # GerÃ§ek 'visits' verisi ile scatter plot
axes[0, 0].plot(df['age'], y_hat, 'o', color='red')  # Tahmin edilen 'visits' verisi (y_hat) kÄ±rmÄ±zÄ± renk ile
axes[0, 0].set_ylabel("visits")  # Y eksenine etiket ekliyoruz (Visits)

# Ä°kinci eksen: Cinsiyet vs Visits (GerÃ§ek veri ve tahminler)
axes[0, 1].plot(df['cinsiyet'], df['visits'], 'o')  # GerÃ§ek 'visits' verisi ile scatter plot
axes[0, 1].plot(df['cinsiyet'], y_hat, 'o', color='red')  # Tahmin edilen 'visits' verisi (y_hat) kÄ±rmÄ±zÄ± renk ile
axes[0, 1].set_ylabel("visits")  # Y eksenine etiket ekliyoruz (Visits)

# ÃœÃ§Ã¼ncÃ¼ eksen: Irk vs Visits (GerÃ§ek veri ve tahminler)
axes[0, 2].plot(df['irk'], df['visits'], 'o')  # GerÃ§ek 'visits' verisi ile scatter plot
axes[0, 2].plot(df['irk'], y_hat, 'o', color='red')  # Tahmin edilen 'visits' verisi (y_hat) kÄ±rmÄ±zÄ± renk ile
axes[0, 2].set_ylabel("visits")  # Y eksenine etiket ekliyoruz (Visits)

# DÃ¶rdÃ¼ncÃ¼ eksen: Kilo vs Visits (GerÃ§ek veri ve tahminler)
axes[1, 0].plot(df['kilo'], df['visits'], 'o')  # GerÃ§ek 'visits' verisi ile scatter plot
axes[1, 0].plot(df['kilo'], y_hat, 'o', color='red')  # Tahmin edilen 'visits' verisi (y_hat) kÄ±rmÄ±zÄ± renk ile
axes[1, 0].set_ylabel("visits")  # Y eksenine etiket ekliyoruz (Visits)

# BeÅŸinci eksen: Boy vs Visits (GerÃ§ek veri ve tahminler)
axes[1, 1].plot(df['boy'], df['visits'], 'o')  # GerÃ§ek 'visits' verisi ile scatter plot
axes[1, 1].plot(df['boy'], y_hat, 'o', color='red')  # Tahmin edilen 'visits' verisi (y_hat) kÄ±rmÄ±zÄ± renk ile
axes[1, 1].set_ylabel("visits")  # Y eksenine etiket ekliyoruz (Visits)

# Alt satÄ±rdaki eksenlerin olduÄŸu gÃ¶rsellerin dÃ¼zeninin daha iyi olmasÄ±nÄ± saÄŸlamak iÃ§in
plt.tight_layout()  # Bu, grafiklerin sÄ±kÄ±ÅŸmamasÄ±nÄ± ve dÃ¼zgÃ¼n yerleÅŸmesini saÄŸlar.

-------------------------------------------------------------

SON OLARAK HEPSÄ°NÄ° SIRALAYALIM:

 		-- Ä°MPORT KISIMLARI --

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn  as sns

---------------------------------------------------------------------

		-- Data Frame Ä°ncelenmesi --
  
df=pd.read_csv("community_health.csv")
sns.pairplot(df)
df.info()
df.isnull().sum()

df.rename(
    columns={"age " : "yas",
             "gender": "cinsiyet",
             "race/ethnicity": "irk",
             "weight":"kilo",
             "height" : "boy"
             }, inplace=True
    )

df["kilo"]  = round(df["kilo"] *0.45)
df["cinsiyet"] = df["cinsiyet"].replace({"female" : 0 , "male" : 1})
df.head(5)

df["irk"] = df["irk"].replace({"group A" :1 ,"group B" : 2 , "group C" : 3 , "group D" :4 ,"group E" : 5})

---------------------------------------------------------------------

		-- TRAÄ°N TEST STPLÄ°T --

x= df.drop("visits" , axis=1)
y= df["visits"]

from sklearn.model_selection import train_test_split

X_train , x_test , y_train , y_test = train_test_split(x,y,test_size=0.33 , random_state=99)

x_test.head()
X_train.head()

from sklearn.linear_model import LinearRegression

regr = LinearRegression()

regr.fit(X_train , y_train)
test_preds = regr.predict(x_test)

from sklearn.metrics import r2_score , mean_absolute_error , mean_squared_error

print("R^2 : ", round(r2_score(y_test,test_preds),2))
print("MAE :", round(mean_absolute_error(y_test,test_preds),2))
print("RMSE :", round(np.sqrt(mean_squared_error(y_test,test_preds)),2))

---------------------------------------------------------------------

		-- Eklenen DeÄŸere Tahmin  --

insan=[[20 , 1,1,80,184 ]]
regr.predict(insan)

y_hat= regr.predict(x)

---------------------------------------------------------------------

		-- Hata Fonksiyonu  --
resudial = y_hat - y

sns.scatterplot(x=y , y= resudial , s=70)
plt.axhline(y=0, color="r" , ls ="-" , lw =4)



