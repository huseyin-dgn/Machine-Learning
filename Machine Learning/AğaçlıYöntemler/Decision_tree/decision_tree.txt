				ğŸŒ³ Decision Tree Nedir?

Bir problemi Ã§Ã¶zmek iÃ§in dallara ayrÄ±larak kararlar alan bir yapay zeka algoritmasÄ±dÄ±r. Her bir dÃ¼ÄŸÃ¼mde (node) bir soru sorulur ve cevaba gÃ¶re farklÄ± dallara (branches) ayrÄ±lÄ±r. ğŸŒ¿

ğŸ“Š NasÄ±l Ã‡alÄ±ÅŸÄ±r?

ğŸŸ¡ KÃ¶k DÃ¼ÄŸÃ¼m (Root Node):  Karar aÄŸacÄ±nÄ±n baÅŸlangÄ±Ã§ noktasÄ±dÄ±r.
ğŸ›¤ï¸ Dallar (Branches): Verinin cevabÄ±na gÃ¶re farklÄ± yollara ayrÄ±lÄ±r.
ğŸŸ¢ Yaprak DÃ¼ÄŸÃ¼mler (Leaf Nodes): SonuÃ§larÄ±n bulunduÄŸu dÃ¼ÄŸÃ¼mlerdir.
ğŸ’¡ Ã–rnek:
Diyelim ki birisi futbol oynayacak mÄ± kararÄ±nÄ± veriyoruz:

ğŸŒ Hava gÃ¼neÅŸliyse â†’ Evet âš½
ğŸŒ§ï¸ YaÄŸmurluysa â†’ HayÄ±r â˜”
â˜ï¸ Bulutluysa â†’ Belki ğŸ¤·
ğŸ“Œ AvantajlarÄ±:
âœ… Kolay anlaÅŸÄ±lÄ±r ve gÃ¶rselleÅŸtirilebilir ğŸ§©
âœ… Hem sÄ±nÄ±flandÄ±rma hem regresyon iÃ§in kullanÄ±labilir ğŸ§ 

âš ï¸ DezavantajlarÄ±:
âŒ Overfitting (AÅŸÄ±rÄ± Ã¶ÄŸrenme) riski var ğŸš¨
âŒ KarÄ±ÅŸÄ±k veriyle fazla bÃ¼yÃ¼yebilir ğŸŒªï¸

ğŸŒŸ SonuÃ§:
Decision Tree, basit ve gÃ¼Ã§lÃ¼ bir algoritmadÄ±r. Hem hÄ±zlÄ± Ã§alÄ±ÅŸÄ±r hem de mantÄ±ÄŸÄ± anlaÅŸÄ±lÄ±r. Ã–zellikle baÅŸlangÄ±Ã§ seviyesinde makine Ã¶ÄŸrenmesi iÃ§in idealdir! ğŸ¤–ğŸ’¯

				ğŸ’¯ BASAMAKLAR ğŸ’¯
1ï¸âƒ£ Veri Ã–n Ä°ÅŸleme (Data Preprocessing)
2ï¸âƒ£ Veri KeÅŸfi ve GÃ¶rselleÅŸtirme (EDA)
3ï¸âƒ£ Korelasyon Analizi (Correlation Analysis)
4ï¸âƒ£ Veri DÃ¶nÃ¼ÅŸÃ¼mÃ¼ (Data Transformation)
5ï¸âƒ£ Train-Test AyrÄ±mÄ± (Train-Test Split)
6ï¸âƒ£ Karar AÄŸacÄ± Modeli (Decision Tree Model)
7ï¸âƒ£ Model DeÄŸerlendirme (Model Evaluation)
8ï¸âƒ£ Model Optimizasyonu (Model Optimization)
9ï¸âƒ£ Fonksiyon ile Model DeÄŸerlendirme (Function for Model Evaluation)
ğŸ”Ÿ Derinlik SÄ±nÄ±rlamasÄ± ile Model (Max Depth Model)
1ï¸âƒ£1ï¸âƒ£ Yaprak SayÄ±sÄ± SÄ±nÄ±rlamasÄ± ile Model (Max Leaf Nodes Model)
1ï¸âƒ£2ï¸âƒ£ Entropi ile Model (Entropy Criterion Model)


## Eksik Veriler
df.info()
df.isnull().sum()
df.dropna(inplace=True) # Null olanlarÄ± sil.
df.isnull().sum()
df["species"].unique()
df["island"].unique()
df["sex"].unique()
df[df["sex"] == "."] # DeÄŸeri karÅŸÄ±mÄ±za Ã§Ä±kardÄ±.
df = df[df["sex"] != "."] 
df[df["sex"] == "."] # DeÄŸeri karÅŸÄ±mÄ±za Ã§Ä±kardÄ±.
df.describe()

âš ï¸Burada yapÄ±lan iÅŸlemler hakkÄ±nda bilgimiz olduÄŸundan yalnÄ±zca bu ÅŸekilde bir aÃ§Ä±klama yapÄ±yorum âš ï¸

			ğŸ–¼ï¸ GÃ¶rselleÅŸtirmeler (Visualizations) ğŸ–¼ï¸	

1ï¸âƒ£ TÃ¼rlerin DaÄŸÄ±lÄ±mÄ± (Species Distribution):

sns.countplot(x="species" , data=df)
AmaÃ§: Veri setindeki penguen tÃ¼rlerinin sayÄ±sÄ±nÄ± gÃ¶sterir. ğŸ“Š
Ã‡Ä±ktÄ±: TÃ¼rlerin (Adelie, Chinstrap, Gentoo) daÄŸÄ±lÄ±mÄ± Ã§ubuk grafiÄŸiyle gÃ¶sterilir.

2ï¸âƒ£ Flipper UzunluÄŸu ve VÃ¼cut AÄŸÄ±rlÄ±ÄŸÄ± Ä°liÅŸkisi:

sns.scatterplot(x="flipper_length_mm" , y="body_mass_g" , data=df, hue="species")
AmaÃ§: Flipper uzunluÄŸu ile vÃ¼cut aÄŸÄ±rlÄ±ÄŸÄ± arasÄ±ndaki iliÅŸkiyi gÃ¶rselleÅŸtirir. ğŸ“ˆ
Ã‡Ä±ktÄ±: TÃ¼rlere gÃ¶re renklenen noktalar, gruplarÄ±n nasÄ±l ayrÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶sterir.

3ï¸âƒ£ Gaga UzunluÄŸu ve Gaga DerinliÄŸi Ä°liÅŸkisi:

sns.scatterplot(x="culmen_length_mm" , y="culmen_depth_mm" , data=df, hue="species")
AmaÃ§: Gaga uzunluÄŸu ve derinliÄŸi arasÄ±ndaki iliÅŸkiyi tÃ¼rlere gÃ¶re analiz eder. ğŸ§
Ã‡Ä±ktÄ±: BazÄ± tÃ¼rlerin gaga Ã¶lÃ§Ã¼mleriyle kolayca ayrÄ±ÅŸtÄ±ÄŸÄ± gÃ¶zlemlenir.

4ï¸âƒ£ Gaga UzunluÄŸu DaÄŸÄ±lÄ±mÄ± (Cinsiyete GÃ¶re):

sns.boxplot(x="species" , y="culmen_length_mm", data=df ,hue="sex")
AmaÃ§: Cinsiyet farkÄ±nÄ±n gaga uzunluÄŸuna etkisini gÃ¶sterir. ğŸ“¦
Ã‡Ä±ktÄ±: Erkek ve diÅŸi penguenlerin gaga uzunluÄŸu karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r.

5ï¸âƒ£ Ã‡ift DeÄŸiÅŸken Ä°liÅŸkileri (Pairplot):

sns.pairplot(data=df , hue="species")
AmaÃ§: TÃ¼m sayÄ±sal deÄŸiÅŸkenler arasÄ±ndaki iliÅŸkileri tek bir grafikte gÃ¶sterir. ğŸ”
Ã‡Ä±ktÄ±: TÃ¼rlere gÃ¶re ayrÄ±ÅŸma olup olmadÄ±ÄŸÄ± gÃ¶zlemlenir.

6ï¸âƒ£ Korelasyon IsÄ± HaritasÄ± (Correlation Heatmap):

sns.heatmap(df.corr(numeric_only=True) , annot=True)
AmaÃ§: SayÄ±sal deÄŸiÅŸkenler arasÄ±ndaki iliÅŸkinin gÃ¼cÃ¼nÃ¼ gÃ¶sterir. ğŸ”¥
Ã‡Ä±ktÄ±: DeÄŸiÅŸkenler arasÄ±ndaki pozitif ve negatif iliÅŸkiler Ä±sÄ± haritasÄ±nda gÃ¶sterilir.
# numeric_only : Dataframe de olan bir hata sonucu yalnÄ±zca sayÄ±sal deÄŸerleri aldÄ±k.

7ï¸âƒ£ Karar AÄŸacÄ± Ã‡izimi (Decision Tree Plot):

plot_tree(model , filled=True, feature_names=x.columns)
AmaÃ§: Karar aÄŸacÄ±nÄ±n nasÄ±l dallandÄ±ÄŸÄ±nÄ± ve hangi deÄŸiÅŸkenlere dayandÄ±ÄŸÄ±nÄ± gÃ¶rselleÅŸtirir. ğŸŒ³
Ã‡Ä±ktÄ±: Karar sÃ¼recini ÅŸematik olarak gÃ¶sterir.
Ä°stersen her bir gÃ¶rselleÅŸtirmenin Ã§Ä±ktÄ±sÄ±nÄ± nasÄ±l yorumlayacaÄŸÄ±nÄ± da anlatabilirim. ğŸ“Šâœ¨

			ğŸ“Š EÄŸitim ve Test Setine AyÄ±rma (Train-Test Split) ğŸ“Š

Train test iÅŸlemine geÃ§meden Ã¶nce bazÄ± konulara deÄŸinelim.3 adet fonksiyonumuz var ve bu fonksiyonlar hakkÄ±nda genel aÃ§Ä±klamalar yapalÄ±m.

ğŸ”„ - get_dummies(), LabelEncoder(), OneHotEncoder() -  ğŸ¤”

Bu Ã¼Ã§ fonksiyon da kategorik verileri sayÄ±sal verilere dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in kullanÄ±lÄ±r, ancak her birinin farklÄ± kullanÄ±m ÅŸekilleri ve avantajlarÄ± vardÄ±r. Åimdi bunlarÄ± inceleyelim:

1ï¸âƒ£ get_dummies()
AÃ§Ä±klama: Pandas kÃ¼tÃ¼phanesinin saÄŸladÄ±ÄŸÄ± bir fonksiyondur.
KullanÄ±m: One-Hot Encoding uygular, yani her kategoriyi bir sÃ¼tuna dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r ve 0/1 deÄŸeri ile kodlar.
Ã–zellik: Kategorinin tÃ¼m farklÄ± deÄŸerleri iÃ§in yeni sÃ¼tunlar oluÅŸturur.

âœ¨ pd.get_dummies(df.drop("species", axis=1), drop_first=True) âœ¨

Parametreler:
1ï¸âƒ£ df.drop("species", axis=1)

AmaÃ§: species sÃ¼tununu veri Ã§erÃ§evesinden Ã§Ä±karÄ±r.
axis=1 ile sÃ¼tun silinir (satÄ±r silmek iÃ§in axis=0 kullanÄ±lÄ±r).
2ï¸âƒ£ drop_first=True

AmaÃ§: Ä°lk kategorik sÃ¼tunu dÃ¼ÅŸÃ¼rÃ¼r.
Neden? Dummy variable trap'i (Ã§oklu doÄŸrusal baÄŸÄ±mlÄ±lÄ±k) engellemek iÃ§in, modelin sadece gerekli sÃ¼tunlarÄ± Ã¶ÄŸrenmesini saÄŸlar.


Ã–rnek:

island = ['Biscoe', 'Dream', 'Torgersen']
island_Biscoe  island_Dream  island_Torgersen  
1               0              0  
0               1              0  
0               0              1

2ï¸âƒ£ LabelEncoder()
AÃ§Ä±klama: Scikit-learn kÃ¼tÃ¼phanesinden gelir ve kategorik verileri tek bir sayÄ±ya dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
KullanÄ±m: Ordinal Encoding uygular, yani her kategoriye sÄ±rasÄ±yla bir sayÄ± atar.
Ã–zellik: Sadece tek sÃ¼tun ile Ã§alÄ±ÅŸÄ±r, kategorik deÄŸerlerin sÄ±rasÄ±z olduÄŸu durumlarda yanlÄ±ÅŸ sonuÃ§ verebilir.


from sklearn.preprocessing import LabelEncoder
data = ['Biscoe', 'Dream', 'Torgersen', 'Dream', 'Biscoe']

# LabelEncoder objesi oluÅŸtur
label_encoder = LabelEncoder()

# Kategorileri sayÄ±lara dÃ¶nÃ¼ÅŸtÃ¼r
encoded_data = label_encoder.fit_transform(data)

print("Orijinal Veri:", data)
print("SayÄ±sal Veriye DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ:", encoded_data)

Ã–rnek:

island = ['Biscoe', 'Dream', 'Torgersen']
SonuÃ§:
island = [0, 1, 2]  
Biscoe â†’ 0, Dream â†’ 1, Torgersen â†’ 2 gibi.

3ï¸âƒ£ OneHotEncoder()
AÃ§Ä±klama: Scikit-learn kÃ¼tÃ¼phanesinde bulunan bir fonksiyondur ve One-Hot Encoding uygular.
KullanÄ±m: Kategorik veriyi sÃ¼tun bazÄ±nda kodlamak iÃ§in kullanÄ±lÄ±r. Pandas get_dummies() ile benzer iÅŸlevi gÃ¶rÃ¼r, ancak genellikle veriler sparse matrix (seyrek matris) ÅŸeklinde dÃ¶ndÃ¼rÃ¼lÃ¼r.
Ã–zellik: Daha esnek ve geniÅŸletilebilir bir yapÄ±ya sahiptir, ancak dÃ¶nÃ¼ÅŸÃ¼m sonrasÄ± Ã§Ä±kan veri tipinin yÃ¶netilmesi gerekebilir.

from sklearn.preprocessing import OneHotEncoder
import numpy as np

# Ã–rnek veri
data = np.array(['Biscoe', 'Dream', 'Torgersen', 'Dream', 'Biscoe']).reshape(-1, 1)

# OneHotEncoder objesi oluÅŸtur
onehot_encoder = OneHotEncoder(sparse=False)

# Kategorileri One-Hot Encoding ile dÃ¶nÃ¼ÅŸtÃ¼r
encoded_data = onehot_encoder.fit_transform(data)

print("Orijinal Veri:", data.flatten())
print("One-Hot Encoded Veri:\n", encoded_data)

Ã–rnek:

island = ['Biscoe', 'Dream', 'Torgersen']
SonuÃ§:
[[1, 0, 0],  
 [0, 1, 0],  
 [0, 0, 1]]


ğŸ”¥ Farklar ve KullanÄ±m DurumlarÄ± ğŸ”¥
get_dummies(): Kolay kullanÄ±m ve hÄ±zlÄ± Ã§Ã¶zÃ¼m saÄŸlar. Genelde pandas ile veri iÅŸleme sÄ±rasÄ±nda tercih edilir.

LabelEncoder(): Sadece tek sÃ¼tun iÃ§in uygundur, sÄ±rasÄ±z kategorilerde problem yaratabilir. Genelde etiketli veriler iÃ§in kullanÄ±lÄ±r.

OneHotEncoder(): Daha esnek ve sparse matrix formatÄ± ile Ã§alÄ±ÅŸÄ±r. BÃ¼yÃ¼k veri setlerinde ve Scikit-learn ile modeller oluÅŸtururken kullanÄ±lÄ±r.

Ã–zet:
Her Ã¼Ã§Ã¼ de kategorik verileri sayÄ±sal verilere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r, fakat kullanÄ±ldÄ±klarÄ± yerler ve saÄŸladÄ±klarÄ± dÃ¶nÃ¼ÅŸÃ¼m ÅŸekilleri farklÄ±dÄ±r. get_dummies() ve OneHotEncoder() aynÄ± iÅŸlevi gÃ¶rÃ¼rken, LabelEncoder() daha farklÄ± bir yaklaÅŸÄ±mla veriyi dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

		ğŸ“Š TRAÄ°N TEST SPLÄ°T Ä°ÅLEMÄ°NE DEVAM EDELÄ°M ğŸ“Š

x= pd.get_dummies(df.drop("species",axis =1), drop_first=True )
y= df["species"]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.3, random_state=9)

Bu yapÄ±lan iÅŸlemler dersimizin baÅŸÄ±ndan beri anlatÄ±lan iÅŸlemler olduÄŸundan , genel aÃ§Ä±klamalara yer vermedim. Åimdi Karar Verme AÄŸacÄ± Modeli'ni inceleyelim.

				ğŸŒ³ Karar Verme AÄŸacÄ± Modeli ğŸŒ³

Model TanÄ±mlamasÄ± ğŸ› ï¸:

DecisionTreeClassifier(): Karar verme aÄŸacÄ± sÄ±nÄ±flandÄ±rma modelini oluÅŸturur.
Bu model, verileri aÄŸaÃ§ yapÄ±sÄ±nda bÃ¶lerek sÄ±nÄ±flandÄ±rÄ±r.

Modeli EÄŸitim Verisiyle EÄŸitme ğŸ“š:

model.fit(X_train, y_train): Modeli eÄŸitim verisiyle eÄŸitir.
X_train: EÄŸitim setinin Ã¶zellikleri (veri).
y_train: EÄŸitim setinin etiketleri (hedef).

Tahmin Yapma ğŸ”®:

pengu_pred = model.predict(X_test): Model, test verisiyle tahmin yapar.
SonuÃ§larÄ± pengu_pred deÄŸiÅŸkeninde saklar.
ğŸ“ˆ Ã–zet:
Model eÄŸitilir ve test verisiyle tahmin yapÄ±lÄ±r. Model, verileri aÄŸaÃ§ ÅŸeklinde bÃ¶lerek kararlar alÄ±r. 

## Karar verme aÄŸacÄ± modeli ## 
 
from sklearn.tree import DecisionTreeClassifier
model= DecisionTreeClassifier()#random_state kullanÄ±labilir.)
model.fit(X_train , y_train)
pengu_pred = model.predict(X_test)

ğŸ“š ğŸ“š Ã–zet ğŸ“š ğŸ“š

Modeli tanÄ±mlarÄ±z â†’ DecisionTreeClassifier()
EÄŸitim verisiyle eÄŸitiriz â†’ model.fit(X_train, y_train)
Test verisiyle tahmin yaparÄ±z â†’ pengu_pred = model.predict(X_test)

			ğŸ” Model DeÄŸerlendirmesi ğŸ”

ğŸ“Š KarÄ±ÅŸÄ±klÄ±k Matrisi (Confusion Matrix) ğŸ“Š

confusion_matrix(): Modelin tahminlerinin doÄŸruluÄŸunu gÃ¶rsel olarak gÃ¶sterir. GerÃ§ek ve tahmin edilen etiketlerin karÅŸÄ±laÅŸtÄ±rÄ±ldÄ±ÄŸÄ± bir matris oluÅŸturur.
ConfusionMatrixDisplay.from_estimator(): KarÄ±ÅŸÄ±klÄ±k matrisini gÃ¶rsel olarak model Ã¼zerinden gÃ¶rÃ¼ntÃ¼ler.

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# KarÄ±ÅŸÄ±klÄ±k Matrisini GÃ¶rselleÅŸtir
ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)

ğŸ“‘ SÄ±nÄ±flandÄ±rma Raporu (Classification Report) ğŸ“‘

classification_report(): Modelin precision, recall, f1-score gibi deÄŸerlendirme metriklerini hesaplar. Bu metrikler, modelin her sÄ±nÄ±f iÃ§in nasÄ±l performans gÃ¶sterdiÄŸini gÃ¶sterir.

from sklearn.metrics import classification_report

# SÄ±nÄ±flandÄ±rma Raporunu YazdÄ±r
print(classification_report(y_test, pengu_pred))

ğŸ”‘ Ã–zellik Ã–nem Dereceleri (Feature Importance) ğŸ”‘

model.feature_importances_: Karar verme aÄŸacÄ±ndaki her bir Ã¶zelliÄŸin model tarafÄ±ndan ne kadar Ã¶nemli olduÄŸunu gÃ¶sterir.
Bu Ã¶zellikler, modelin kararlarÄ±nÄ± vermesinde ne kadar etkili olduÄŸunu belirtir.

# Ã–zelliklerin Ã–nem Derecelerini GÃ¶rÃ¼ntÃ¼le
print(model.feature_importances_)

ğŸ“Š Ã–zelliklerin Ã–nem Derecelerini GÃ¶rselleÅŸtirme ğŸ“Š

pd.DataFrame(): Ã–zelliklerin Ã¶nem derecelerini veri Ã§erÃ§evesi (DataFrame) olarak gÃ¶sterir. Her Ã¶zelliÄŸin modeldeki Ã¶nemini daha net gÃ¶rmemizi saÄŸlar.

import pandas as pd

# Ã–zelliklerin Ã–nem Derecelerini GÃ¶rselleÅŸtir
pd.DataFrame(index=X.columns, data=model.feature_importances_, columns=["KatsayÄ±lar"])

ğŸ“š ğŸ“š Ã–zet ğŸ“š ğŸ“š

KarÄ±ÅŸÄ±klÄ±k Matrisini gÃ¶rselleÅŸtirerek modelin doÄŸruluÄŸunu inceliyoruz.
SÄ±nÄ±flandÄ±rma raporunu yazdÄ±rarak modelin performansÄ±nÄ± daha detaylÄ± deÄŸerlendiriyoruz.
Ã–zelliklerin Ã¶nem derecelerini Ã¶ÄŸrenerek modelin hangi Ã¶zelliklere odaklandÄ±ÄŸÄ±nÄ± analiz ediyoruz.


Burda yapÄ±lan iÅŸlemler logistic regresyon tanÄ±mÄ±nda da iÅŸimize yaramÄ±ÅŸtÄ±.

		ğŸ” Karar Verme AÄŸacÄ± Modelinin Ä°ncelenmesi ve Optimizasyonu ğŸŒ³

 1ï¸âƒ£ Karar Verme AÄŸacÄ±nÄ±n GÃ¶rselleÅŸtirilmesi ğŸŒ³

plot_tree()
Karar verme aÄŸacÄ±nÄ± gÃ¶rselleÅŸtiren fonksiyondur. Bu fonksiyon, modelin yapÄ±sÄ±nÄ± aÄŸaÃ§ ÅŸeklinde Ã§izmenizi saÄŸlar. AÄŸacÄ±n her dalÄ± bir Ã¶zellik (feature), her yaprak ise bir sonuÃ§ (class label) temsil eder.

from sklearn.tree import plot_tree

# Ä°lk gÃ¶rselleÅŸtirme: Grafik boyutunu ayarlayarak aÄŸacÄ± Ã§iz
plt.figure(figsize=(10,8))
plot_tree(model)

# Ä°kinci gÃ¶rselleÅŸtirme: Daha yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼k ve daha bÃ¼yÃ¼k boyutla aÄŸacÄ± Ã§iz
plt.figure(figsize=(12,8), dpi=200)
plot_tree(model, filled=True, feature_names=x.columns)
Parametreler:

model: Karar aÄŸacÄ±nÄ±z.
figsize: Grafik boyutunu ayarlamak iÃ§in kullanÄ±lÄ±r.
dpi: Ã‡Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼ ayarlamak iÃ§in kullanÄ±lÄ±r.
filled=True: AÄŸacÄ±n dÃ¼ÄŸÃ¼mlerini renkli doldurur, hangi sÄ±nÄ±fÄ±n hakim olduÄŸunu gÃ¶sterir.
feature_names: Ã–zelliklerin adlarÄ±nÄ± kullanarak aÄŸacÄ± etiketler.

2ï¸âƒ£ Fonksiyon OluÅŸturma (Model SonuÃ§larÄ±nÄ± GÃ¶rselleÅŸtirme ve DeÄŸerlendirme) ğŸ“ˆ

model_sonuc()
Bu fonksiyon, modelin tahminini yapar, deÄŸerlendirme metriklerini yazdÄ±rÄ±r ve karar aÄŸacÄ±nÄ± gÃ¶rselleÅŸtirir. Fonksiyon, karar aÄŸacÄ±nÄ±n Ã§eÅŸitli versiyonlarÄ±nÄ± deÄŸerlendirmemize yardÄ±mcÄ± olur.

def model_sonuc(model):
    # Modeli test verisiyle tahmin yapacak ÅŸekilde kullan
    pengu_pred = model.predict(X_test)
    
    # KarÄ±ÅŸÄ±klÄ±k matrisini gÃ¶rselleÅŸtir
    ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)
    
    # SÄ±nÄ±flandÄ±rma raporunu yazdÄ±r
    print(classification_report(y_test, pengu_pred))
    
    # Karar aÄŸacÄ±nÄ± gÃ¶rselleÅŸtir
    plt.figure(figsize=(12,8), dpi=200)
    plot_tree(model, filled=True, feature_names=x.columns)

Parametreler:

model: EÄŸitilmiÅŸ karar aÄŸacÄ± modelini alÄ±r.

pengu_pred: Test verisi Ã¼zerinde tahmin yapÄ±lan sonuÃ§larÄ± tutar.

ConfusionMatrixDisplay.from_estimator(): Modelin doÄŸruluÄŸunu gÃ¶rselleÅŸtiren karÄ±ÅŸÄ±klÄ±k matrisini gÃ¶sterir.

classification_report(): Modelin performansÄ±nÄ± precision, recall, f1-score gibi metriklerle deÄŸerlendirir.

plot_tree(): Karar aÄŸacÄ±nÄ± gÃ¶rselleÅŸtirir.

3ï¸âƒ£ Az Derinlikli Karar AÄŸacÄ± (Pruning) ğŸƒ

max_depth=2
Karar aÄŸacÄ±nÄ±n derinliÄŸini sÄ±nÄ±rlamak iÃ§in kullanÄ±lÄ±r. Derinlik arttÄ±kÃ§a modelin karmaÅŸÄ±klÄ±ÄŸÄ± ve aÅŸÄ±rÄ± uyum yapma (overfitting) ihtimali artar. Bu parametre, aÄŸacÄ±n daha basit hale gelmesini saÄŸlar.

az_agac = DecisionTreeClassifier(max_depth=2)
az_agac.fit(X_train, y_train)
model_sonuc(az_agac)

Parametreler:

max_depth=2: Karar aÄŸacÄ±nÄ±n derinliÄŸini 2 ile sÄ±nÄ±rlÄ±yoruz. Bu, aÄŸacÄ±n fazla karmaÅŸÄ±klaÅŸmasÄ±nÄ± Ã¶nler.

4ï¸âƒ£ Az YapraklÄ± Karar AÄŸacÄ± ğŸ‚

max_leaf_nodes=5
Bu parametre, aÄŸacÄ±n sahip olacaÄŸÄ± maksimum yaprak (sonuÃ§) sayÄ±sÄ±nÄ± sÄ±nÄ±rlamak iÃ§in kullanÄ±lÄ±r. AÄŸacÄ±n daha basit ve genellenebilir olmasÄ±nÄ± saÄŸlar.

az_yaprak = DecisionTreeClassifier(max_leaf_nodes=5)
az_yaprak.fit(X_train, y_train)
model_sonuc(az_yaprak)
Parametreler:

max_leaf_nodes=5: AÄŸacÄ±n en fazla 5 yapraÄŸa sahip olmasÄ±nÄ± saÄŸlar. Bu da modelin daha basit olmasÄ±na yardÄ±mcÄ± olur.

5ï¸âƒ£ Karar AÄŸacÄ±nÄ±n Optimizasyonu (Entropy KullanÄ±mÄ±) âš™ï¸

criterion="entropy"
Bu parametre, karar aÄŸacÄ±nÄ±n nasÄ±l oluÅŸturulacaÄŸÄ±nÄ± belirler. "entropy" kriteri, bilgiyi kazancÄ± en yÃ¼ksek olan bÃ¶lmeleri seÃ§er. DiÄŸer seÃ§enek ise "gini" dir, bu da daha hÄ±zlÄ± Ã§alÄ±ÅŸabilir ancak bazÄ± durumlarda daha az doÄŸru olabilir.

entropi = DecisionTreeClassifier(criterion="entropy")
entropi.fit(X_train, y_train)
model_sonuc(entropi)
Parametreler:

criterion="entropy": Bu parametre, karar aÄŸacÄ±nÄ±n oluÅŸumunda kullanÄ±lan Ã¶lÃ§Ã¼mdÃ¼r. Entropy (bilgi kazanÄ±mÄ±) kullanÄ±larak aÄŸacÄ±n bÃ¶lÃ¼nmesi saÄŸlanÄ±r.

ğŸ“š ğŸ“š Ã–zet ğŸ“š ğŸ“š

plot_tree(): Karar aÄŸacÄ±nÄ± gÃ¶rselleÅŸtirir.
model_sonuc(): Modelin performansÄ±nÄ± deÄŸerlendirir, karÄ±ÅŸÄ±klÄ±k matrisi ve sÄ±nÄ±flandÄ±rma raporu saÄŸlar.
max_depth ve max_leaf_nodes: Modelin karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± kontrol eder, aÅŸÄ±rÄ± uyumu engeller.
criterion="entropy": Karar aÄŸacÄ±nÄ±n bilgi kazanÄ±mÄ±nÄ± optimize eder.

	   ğŸ§ ğŸ’¡ Karar AÄŸaÃ§larÄ±: Entropi ve Gini Kriterleri ile Veri KÃ¼mesi BÃ¶lme ğŸ§ ğŸ’¡


Karar aÄŸacÄ± oluÅŸtururken, modelin nasÄ±l bÃ¶lÃ¼neceÄŸini belirlemek iÃ§in kullanÄ±lan farklÄ± kriterler vardÄ±r. Ä°ki yaygÄ±n kriter "entropy" ve **"gini"**dir. Bunlar, aÄŸacÄ±n dallanmasÄ±nda kullanÄ±lan matematiksel Ã¶lÃ§Ã¼tlerdir. Ä°ÅŸte bu kriterlerin aÃ§Ä±klamalarÄ±:

1ï¸âƒ£ Entropy (Bilgi KazancÄ±)
AÃ§Ä±klama: Entropi, veri kÃ¼mesindeki belirsizliÄŸi Ã¶lÃ§er. Entropi deÄŸerinin dÃ¼ÅŸÃ¼k olmasÄ±, veri kÃ¼mesinin daha homojen olduÄŸu anlamÄ±na gelir. YÃ¼ksek entropi ise daha fazla Ã§eÅŸitlilik ve belirsizlik iÃ§erdiÄŸini gÃ¶sterir. Karar aÄŸacÄ±, her bir dÃ¼ÄŸÃ¼mde entropiyi minimize ederek daha homojen gruplar elde etmeye Ã§alÄ±ÅŸÄ±r.


AvantajlarÄ±:

Genellikle daha doÄŸru sonuÃ§lar verir.
Verinin iÃ§indeki tÃ¼m Ã§eÅŸitliliÄŸi dikkate alÄ±r.

DezavantajlarÄ±:

Hesaplama aÃ§Ä±sÄ±ndan biraz daha yavaÅŸ olabilir, Ã§Ã¼nkÃ¼ daha karmaÅŸÄ±k bir hesaplama gerektirir.

2ï¸âƒ£ Gini Impurity (Gini SafsÄ±zlÄ±ÄŸÄ±)

AÃ§Ä±klama: Gini, her iki sÄ±nÄ±fÄ±n da birbirine karÄ±ÅŸma olasÄ±lÄ±ÄŸÄ±nÄ± Ã¶lÃ§en bir Ã¶lÃ§Ã¼ttÃ¼r. YÃ¼ksek Gini deÄŸeri, verinin daha karÄ±ÅŸÄ±k olduÄŸunu gÃ¶sterirken, dÃ¼ÅŸÃ¼k bir Gini deÄŸeri veri kÃ¼mesinin daha saf olduÄŸunu gÃ¶sterir. Bu metrik, veri kÃ¼mesini daha homojen gruplara ayÄ±rmak iÃ§in kullanÄ±lÄ±r.

AvantajlarÄ±:

Daha hÄ±zlÄ± hesaplanÄ±r, bu nedenle bÃ¼yÃ¼k veri setlerinde daha verimli olabilir.
Genellikle hÄ±zlÄ± ve etkili sonuÃ§lar saÄŸlar.

DezavantajlarÄ±:

Entropi kadar hassas olmayabilir, Ã¶zellikle karmaÅŸÄ±k verilerde.

3ï¸âƒ£ Best Split

AÃ§Ä±klama: Bu kriter, verilen her Ã¶zellik iÃ§in veri kÃ¼mesinin en iyi ÅŸekilde nasÄ±l bÃ¶lÃ¼neceÄŸine karar verir. En iyi bÃ¶lme, veri kÃ¼mesindeki entropi veya Gini impurity deÄŸerini minimize eden bÃ¶lme olarak kabul edilir.
AvantajlarÄ±:
Basit ve genellikle anlaÅŸÄ±lÄ±r sonuÃ§lar elde edilir.


		ğŸŒ³ Karar Verme AÄŸaÃ§larÄ±nÄ±n DiÄŸer YÃ¶ntemlerden FarkÄ± ğŸŒ³

1ï¸âƒ£ Hikayeye DayalÄ± Modelleme (Yorumlanabilirlik) ğŸ’¬
Logistic Regression ve Linear Regression gibi yÃ¶ntemler, doÄŸrusal iliÅŸkileri modellemek iÃ§in kullanÄ±lÄ±r. Yani, bu modellerde baÄŸÄ±msÄ±z deÄŸiÅŸkenlerin (features) baÄŸÄ±mlÄ± deÄŸiÅŸkenle olan iliÅŸkisi bir doÄŸru ile temsil edilir. Bu tÃ¼r modeller genellikle daha basit ve sayÄ±sal Ã§Ã¶zÃ¼mler sunar.
Karar AÄŸaÃ§larÄ± ise Ã§ok daha yorumlanabilir ve gÃ¶rselleÅŸtirilebilir. Her bir dallanma, bir soruyu temsil eder ve bu sorulara verdiÄŸimiz yanÄ±tlar (evet/hayÄ±r) kararlarÄ± belirler. Bu yÃ¼zden karar aÄŸacÄ±nÄ±n Ã§Ä±ktÄ±larÄ± bir "karar verme sÃ¼reci" gibi gÃ¶rÃ¼lebilir. Yani, modelin nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± kolayca aÃ§Ä±klayabilirsiniz.

2ï¸âƒ£ Non-lineer Ä°liÅŸkiler ğŸŒ
Logistic regression ve linear regression gibi yÃ¶ntemler, doÄŸrusal iliÅŸkiler Ã¼zerine Ã§alÄ±ÅŸÄ±r. Yani, baÄŸÄ±msÄ±z deÄŸiÅŸkenler ile baÄŸÄ±mlÄ± deÄŸiÅŸken arasÄ±ndaki iliÅŸki bir dÃ¼z Ã§izgi ile modellenir.
Karar AÄŸaÃ§larÄ±, non-lineer iliÅŸkileri modellemek konusunda Ã§ok daha gÃ¼Ã§lÃ¼dÃ¼r. Yani, bir deÄŸiÅŸkenin diÄŸerine etkisi doÄŸrusal olmayabilir, ve karar aÄŸaÃ§larÄ± bu tÃ¼r karmaÅŸÄ±k iliÅŸkileri Ã§ok kolay bir ÅŸekilde yakalayabilir.

3ï¸âƒ£ Ã–zellik SeÃ§imi ve Ã–nem Derecesi (Feature Importance) ğŸ”
DiÄŸer bazÄ± modellerde, Ã¶zelliklerin model Ã¼zerindeki etkisini anlamak zor olabilir. Ã–rneÄŸin, logistic regression'da p-deÄŸerleri ile anlamlÄ±lÄ±k kontrol edilir, ancak Ã¶zelliklerin Ã¶nem derecelerini doÄŸrudan gÃ¶remeyebilirsiniz.
Karar AÄŸaÃ§larÄ± ise, her bir Ã¶zelliÄŸi aÄŸaÃ§ yapÄ±sÄ±nda dallanma noktasÄ± olarak gÃ¶sterdiÄŸi iÃ§in, hangi Ã¶zelliklerin karar Ã¼zerinde daha fazla etkisi olduÄŸunu aÃ§Ä±kÃ§a gÃ¶rebiliriz. Bu Ã¶zelliklerin Ã¶nem derecelerini de feature importance ile deÄŸerlendirebiliriz.

4ï¸âƒ£ Veri Preprocessing ve Ã–zellik DÃ¶nÃ¼ÅŸÃ¼mÃ¼ ğŸ”„
Logistic regression ve linear regression gibi doÄŸrusal modeller, genellikle verilerin normalizasyonu, kategorik deÄŸiÅŸkenlerin encoding iÅŸlemleri gibi ekstra adÄ±mlar gerektirir.
Karar AÄŸaÃ§larÄ± bu iÅŸlemlere Ã§ok daha dayanÄ±klÄ±dÄ±r. Ã–rneÄŸin, kategorik deÄŸiÅŸkenler (Ã¶rneÄŸin "island" veya "sex") doÄŸrudan kullanÄ±labilir ve normalizasyon gerekmez. Karar aÄŸaÃ§larÄ± veriyi olduÄŸu gibi iÅŸleyebilir.

5ï¸âƒ£ AÅŸÄ±rÄ± Uyum (Overfitting) ğŸŒªï¸
Logistic regression gibi yÃ¶ntemler daha az parametre iÃ§erdiÄŸi iÃ§in aÅŸÄ±rÄ± uyum riski daha dÃ¼ÅŸÃ¼ktÃ¼r, fakat Ã§ok karmaÅŸÄ±k verilerle Ã§alÄ±ÅŸÄ±rken doÄŸrusal olmayan iliÅŸkiler yeterince iyi yakalanamayabilir.
Karar AÄŸaÃ§larÄ±, doÄŸru ÅŸekilde yapÄ±landÄ±rÄ±lmadÄ±ÄŸÄ±nda aÅŸÄ±rÄ± uyum yapma riski taÅŸÄ±yabilir. Ancak, derinlik (depth) veya yaprak sayÄ±sÄ± gibi parametrelerle bu riski kontrol altÄ±na almak mÃ¼mkÃ¼ndÃ¼r.

ğŸ“ŠğŸ“ŠğŸ“Š Ã–zetle ğŸ“ŠğŸ“ŠğŸ“Š;

Logistic regression gibi modeller doÄŸrusal iliÅŸkiler kurarken, karar aÄŸaÃ§larÄ± daha esnek ve non-lineer iliÅŸkileri modelleyebilir.
Karar aÄŸaÃ§larÄ± yorumlanabilirliÄŸi arttÄ±rÄ±r ve gÃ¶rselleÅŸtirilmesi daha kolaydÄ±r. AyrÄ±ca, hangi Ã¶zelliklerin daha Ã¶nemli olduÄŸunu net bir ÅŸekilde belirlemek mÃ¼mkÃ¼ndÃ¼r.
SonuÃ§ olarak, karar aÄŸaÃ§larÄ± genellikle karmaÅŸÄ±k ve Ã§ok deÄŸiÅŸkenli verilerle Ã§alÄ±ÅŸÄ±rken tercih edilir, Ã§Ã¼nkÃ¼ iliÅŸkiler doÄŸrusal olmayan durumlarÄ± daha iyi Ã¶ÄŸrenebilirler.

	ğŸªµ Karar Verme AÄŸaÃ§larÄ± (Decision Trees) ile Modelleme ve GÃ¶rselleÅŸtirme ğŸªµ

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Veriyi oku
df = pd.read_csv("penguins.csv")
df.head()

# Eksik verileri kontrol et ve temizle
df.info()
df.isnull().sum()
df.dropna(inplace=True)

# Kategorik deÄŸiÅŸkenleri kontrol et
df["species"].unique()
df["island"].unique()
df["sex"].unique()

# sex sÃ¼tununda eksik veriyi temizle
df = df[df["sex"] != "."]
df.describe()

# GÃ¶rselleÅŸtirmeler
sns.countplot(x="species", data=df)
sns.scatterplot(x="flipper_length_mm", y="body_mass_g", data=df, hue="species")
sns.scatterplot(x="culmen_length_mm", y="culmen_depth_mm", data=df, hue="species")
sns.boxplot(x="species", y="culmen_length_mm", data=df, hue="sex")
sns.heatmap(df.corr(numeric_only=True), annot=True)

# Veri dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (One-Hot Encoding)
x = pd.get_dummies(df.drop("species", axis=1), drop_first=True)
y = df["species"]

# EÄŸitim ve test setine ayÄ±rma
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=9)

# Karar verme aÄŸacÄ± modeli
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(random_state=9)
model.fit(X_train, y_train)

# Test setiyle tahmin yap
pengu_pred = model.predict(X_test)

# Model DeÄŸerlendirme
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
confusion_matrix(y_test, pengu_pred)
ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)
print(classification_report(y_test, pengu_pred))

# Ã–zelliklerin Ã¶nemini gÃ¶ster
model.feature_importances_
pd.DataFrame(index=x.columns, data=model.feature_importances_, columns=["KatsayÄ±lar"])

# Karar aÄŸacÄ±nÄ± gÃ¶rselleÅŸtir
from sklearn.tree import plot_tree
plt.figure(figsize=(10, 8))
plot_tree(model)

plt.figure(figsize=(12, 8), dpi=200)
plot_tree(model, filled=True, feature_names=x.columns)

# Model SonuÃ§larÄ±nÄ± Fonksiyon Haline Getirme
def model_sonuc(model):
    pengu_pred = model.predict(X_test)
    ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)
    print(classification_report(y_test, pengu_pred))
    plt.figure(figsize=(12, 8), dpi=200)
    plot_tree(model, filled=True, feature_names=x.columns)

# Az Derin AÄŸaÃ§
az_agac = DecisionTreeClassifier(max_depth=2)
az_agac.fit(X_train, y_train)
model_sonuc(az_agac)

# Az YapraklÄ± AÄŸaÃ§
az_yaprak = DecisionTreeClassifier(max_leaf_nodes=5)
az_yaprak.fit(X_train, y_train)
model_sonuc(az_yaprak)

# Optimizasyon (Entropi)
entropi = DecisionTreeClassifier(criterion="entropy")
entropi.fit(X_train, y_train)
model_sonuc(entropi)





















