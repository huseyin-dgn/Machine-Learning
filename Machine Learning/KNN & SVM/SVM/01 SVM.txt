				ğŸ¤–ğŸ“Š Destek VektÃ¶r Makineleri (SVM) Nedir? ğŸ¤–ğŸ“Š

SVM, sÄ±nÄ±flandÄ±rma (classification) ve regresyon (regression) problemlerinde kullanÄ±lan gÃ¼Ã§lÃ¼ bir makine Ã¶ÄŸrenmesi algoritmasÄ±dÄ±r. Temel amacÄ±, veri noktalarÄ±nÄ± en iyi ÅŸekilde ayÄ±ran bir hiper dÃ¼zlem (hyperplane) bulmaktÄ±r. ğŸš€

				ğŸ¤” SVM Neden KullanÄ±lÄ±r? ğŸ¤”

âœ… KarmaÅŸÄ±k Verilerde GÃ¼Ã§lÃ¼dÃ¼r â€“ Veri kÃ¼mesi Ã§ok boyutlu ve karmaÅŸÄ±ksa bile iyi sonuÃ§lar verir.
âœ… KÃ¼Ã§Ã¼k Veri Setlerinde Ä°yi Ã‡alÄ±ÅŸÄ±r â€“ Ã‡ok fazla veri gerektirmeden etkili olabilir.
âœ… AykÄ±rÄ± DeÄŸerlere DayanÄ±klÄ±dÄ±r â€“ Destek vektÃ¶rleri sayesinde veri setindeki gereksiz gÃ¼rÃ¼ltÃ¼lerden daha az etkilenir.
âœ… Ã‡ekirdek (Kernel) Trikleriyle â€“ DoÄŸrusal olmayan (nonlinear) verileri bile ayÄ±rabilir. (Ã–rn: RBF, Polinomial Ã§ekirdekler)

				ğŸŒ² SVM ğŸ¤– vs. Decision Tree ğŸŒ²

Ã‡alÄ±ÅŸma MantÄ±ÄŸÄ±:

SVM ğŸ¤–: Hiper dÃ¼zlemle verileri en iyi ÅŸekilde ayÄ±rmaya Ã§alÄ±ÅŸÄ±r. ğŸ“
Decision Tree ğŸŒ²: Veriyi dallara bÃ¶lerek, if-else mantÄ±ÄŸÄ±yla karar verir. ğŸŒ³
Ã–lÃ§eklendirme:

SVM ğŸ¤–: Genellikle gereklidir. ğŸ”„
Decision Tree ğŸŒ²: Ã‡oÄŸu zaman gerekmez. ğŸš«
AykÄ±rÄ± DeÄŸerlere DuyarlÄ±lÄ±k:

SVM ğŸ¤–: Daha az duyarlÄ±dÄ±r, destek vektÃ¶rleri sayesinde aykÄ±rÄ± deÄŸerlere dayanÄ±klÄ±dÄ±r. ğŸ‘
Decision Tree ğŸŒ²: AykÄ±rÄ± deÄŸerlere karÅŸÄ± daha hassastÄ±r ve aÅŸÄ±rÄ± Ã¶ÄŸrenmeye meyillidir. ğŸ˜•
Ã–ÄŸrenme Åekli:

SVM ğŸ¤–: KÃ¼Ã§Ã¼k veri setlerinde gÃ¼Ã§lÃ¼dÃ¼r ve iyi genelleme yapar. ğŸ“Š
Decision Tree ğŸŒ²: BÃ¼yÃ¼k veri setlerinde daha hÄ±zlÄ± Ã§alÄ±ÅŸÄ±r ve veri arttÄ±kÃ§a performansÄ± daha iyidir. ğŸš€
Model KarmaÅŸÄ±klÄ±ÄŸÄ±:

SVM ğŸ¤–: Daha matematiksel ve karmaÅŸÄ±ktÄ±r, Ã§ekirdek (kernel) fonksiyonlarÄ± kullanarak doÄŸrusal olmayan verileri de ayrÄ±ÅŸtÄ±rabilir. ğŸ“
Decision Tree ğŸŒ²: Daha sezgisel ve yorumlanabilir, aÄŸaÃ§ yapÄ±sÄ± sayesinde kolay anlaÅŸÄ±lÄ±r. ğŸ§

ğŸ”¹ Ã–zetle:

SVM, Ã¶zellikle yÃ¼ksek boyutlu ve karmaÅŸÄ±k verilerde daha iyi genelleme yaparken, Decision Tree daha hÄ±zlÄ± Ã§alÄ±ÅŸÄ±r ve yorumlamasÄ± daha kolaydÄ±r.
AykÄ±rÄ± deÄŸerlere karÅŸÄ± dayanÄ±klÄ± bir model istersen SVM tercih edebilirsin.
HÄ±z ve aÃ§Ä±klanabilirlik Ã¶n plandaysa Decision Tree daha iyi bir seÃ§enek olabilir.

					ğŸ› ï¸ SVM AdÄ±mlarÄ±

ğŸ“Œ 1. Veri HazÄ±rlama & KeÅŸifsel Veri Analizi (EDA)
ğŸ“Œ 2. SVM Modelinin KurulmasÄ±
ğŸ“Œ 3. C Parametresinin Etkisi
ğŸ“Œ 4. Kernel TÃ¼rleri ve Etkileri
ğŸ”¹ Linear Kernel
ğŸ”¹ RBF Kernel
ğŸ”¹ Sigmoid Kernel
ğŸ”¹ Polynomial Kernel
ğŸ“Œ 5. Gamma Parametresinin Etkisi (RBF Kernel)
ğŸ“Œ 6. Grid Search ile En Ä°yi Modeli Bulma
ğŸ“Œ 7. Final Model ve DeÄŸerlendirme


				ğŸ“Œ 1. Veri HazÄ±rlama & KeÅŸifsel Veri Analizi (EDA)

Normalden farklÄ± bir veri hazÄ±rlama iÅŸlemi yapmÄ±yoruz.DiÄŸer txt dosyalarÄ±ndan eriÅŸebilirsiniz.YalnÄ±zca yeni bir gÃ¶rselleÅŸtirme ekleyeceÄŸiz.DosyalarÄ±n iÃ§erisinde bulunan py dosyalarÄ± bizim gÃ¶rÃ¼ntÃ¼ ile anlamamÄ±zÄ± saÄŸlayacak.Åu ÅŸekilde importluyoruz.

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from svm_margin_plot import plot_svm_boundary

sns.scatterplot(x="Limon" ,y="Sirke", data=df , hue="TurÅŸu" , palette="bright")


				ğŸ“Œ 2. SVM Modelinin KurulmasÄ±

Model kurulumlarÄ±nÄ±n iÃ§ taraflarÄ±na girdiÄŸimizde bazÄ± kernel , c ve gamma deÄŸerleri bizi karÅŸÄ±layacak.

X = df.drop("TurÅŸu", axis=1)
y = df["TurÅŸu"]

from sklearn.svm import SVC
# support vector machine : svm
# support vector classifier: svc

from svm_margin_plot import plot_svm_boundary

Kernel TÃ¼rleri

Linear (kernel="linear"): Veriler doÄŸrusal olarak ayrÄ±labiliyorsa kullanÄ±lÄ±r.
RBF (kernel="rbf"): DoÄŸrusal olmayan verilerde kullanÄ±lÄ±r, gamma parametresi ile kontrol edilir.
Sigmoid (kernel="sigmoid"): Sinir aÄŸÄ± benzeri bir yaklaÅŸÄ±m saÄŸlar, doÄŸrusal olmayan ayrÄ±mlarda kullanÄ±lÄ±r.
Polynomial (kernel="poly"): Verileri polinom derecesine gÃ¶re ayÄ±rÄ±r, degree parametresi ile kontrol edilir.

C (Penalty) Parametresi
BÃ¼yÃ¼k C: Hata toleransÄ± dÃ¼ÅŸÃ¼k, marjin dar, overfitting riski yÃ¼ksek.
KÃ¼Ã§Ã¼k C: Hata toleransÄ± yÃ¼ksek, marjin geniÅŸ, underfitting riski olabilir.

Gamma (Î³) Parametresi
KÃ¼Ã§Ã¼k Gamma: Daha genel karar sÄ±nÄ±rlarÄ±, dÃ¼ÅŸÃ¼k varyans, yÃ¼ksek bias, underfitting riski.
BÃ¼yÃ¼k Gamma: Daha detaylÄ± ve hassas karar sÄ±nÄ±rlarÄ±, yÃ¼ksek varyans, dÃ¼ÅŸÃ¼k bias, overfitting riski.


				ğŸ“Œ3. Lineer Model ve C Parametresi

	ğŸ“Œ Lineer Model ve C Parametresi

ğŸ“Œ Lineer SVM Modeli
svc_model = SVC(kernel="linear", C=1)  
svc_model.fit(X, y)
  
ğŸ“Œ C=1 VarsayÄ±lan DeÄŸer

ğŸ”¹ SVM'deki C Parametresi: Marjin geniÅŸliÄŸi ve sÄ±nÄ±flandÄ±rma hatasÄ± arasÄ±ndaki dengeyi kontrol eder.
âœ”ï¸ C=1: Dengeli bir marjin saÄŸlar.
âœ”ï¸ Kesik Ã§izgiler (support vectors) daha geniÅŸ bir alana yayÄ±lÄ±r.
âœ”ï¸ Model, bazÄ± hatalara izin vererek genelleme gÃ¼cÃ¼nÃ¼ artÄ±rÄ±r.


ğŸ“Œ C = 324

svc_model = SVC(kernel="linear", C=324)
svc_model.fit(X,y)

âš ï¸ C=324: Marjin daralÄ±r, model veriye daha sÄ±kÄ± uyar.
âš ï¸ Kesik Ã§izgiler (support vectors) birbirine daha yaklaÅŸÄ±r.
âš ï¸ Model, sÄ±nÄ±rlara yakÄ±n noktalarÄ± daha hassas bir ÅŸekilde hesaba katar.


ğŸ“Œ C Parametresinin Etkisi

ğŸ”µ BÃ¼yÃ¼k C (Ã¶rn: C=1000)
âœ”ï¸ Hata toleransÄ± dÃ¼ÅŸÃ¼k â†’ Model veriye daha fazla uyar.
âš ï¸ DÃ¼ÅŸÃ¼k bias, yÃ¼ksek varyans â†’ Overfitting riski artar.
ğŸ“ Marjin daralÄ±r, karar sÄ±nÄ±rlarÄ± katÄ± hale gelir.
ğŸ¯ GÃ¼rÃ¼ltÃ¼lÃ¼ verilere karÅŸÄ± daha duyarlÄ±dÄ±r.

ğŸŸ¢ KÃ¼Ã§Ã¼k C (Ã¶rn: C=0.1)

ğŸ“ Daha geniÅŸ marjin â†’ BazÄ± yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmalara izin verir.
âš ï¸ Daha yÃ¼ksek bias, dÃ¼ÅŸÃ¼k varyans â†’ Underfitting riski olabilir.
ğŸ”„ Modelin genelleme gÃ¼cÃ¼ artar.
ğŸ’ª GÃ¼rÃ¼ltÃ¼lÃ¼ verilere karÅŸÄ± daha dayanÄ±klÄ±dÄ±r.


ğŸ“Œ Câ€™nin Grafikteki Etkisi

plot_svm_boundary(svc_model, X, y)

ğŸ¨ Grafikte C=1: Orta seviyede bir marjin geniÅŸliÄŸi saÄŸlar.

ğŸ“Œ BÃ¼yÃ¼k C KullanÄ±mÄ±

svc_model = SVC(kernel="linear", C=324)
svc_model.fit(X, y)  
plot_svm_boundary(svc_model, X, y)

ğŸ“‰ C BÃ¼yÃ¼dÃ¼kÃ§e: Karar sÄ±nÄ±rlarÄ± sÄ±kÄ±laÅŸÄ±r, marjin daralÄ±r.

ğŸ“Œ KÃ¼Ã§Ã¼k C KullanÄ±mÄ±

svc_model = SVC(kernel="linear", C=0.01)
svc_model.fit(X, y)  
plot_svm_boundary(svc_model, X, y)

ğŸ“ˆ C KÃ¼Ã§Ã¼ldÃ¼kÃ§e: Karar sÄ±nÄ±rlarÄ± geniÅŸler, marjin bÃ¼yÃ¼r.

ğŸ“Œ Ã–zet:
âœ… C bÃ¼yÃ¼kse â†’ Daha kesin sÄ±nÄ±flandÄ±rma, overfitting riski artar.
âœ… C kÃ¼Ã§Ã¼kse â†’ Daha geniÅŸ marjin, underfitting riski olabilir.


				ğŸ“Œ4. Kernel TÃ¼rleri ve Etkileri

	ğŸ“Œ RBF (Radial Basis Function) Kernel

	ğŸ“Œ Sigmoid Kernel

	ğŸ“Œ Polynomial Kernel


ğŸ“Œ RBF (Radial Basis Function) Kernel

svm_model = SVC(kernel="rbf", gamma=0.3)
svm_model.fit(X,y)
plot_svm_boundary(svm_model, X,y)

RBF Kernel: Kernel yÃ¶ntemlerinden biri olan Radial Basis Function, doÄŸrusal olarak ayrÄ±lamayan, karmaÅŸÄ±k verilere uygulanmak iÃ§in kullanÄ±lÄ±r. SVM'deki doÄŸrusal olmayan ayrÄ±mlarda sÄ±klÄ±kla tercih edilir.

Gauss (Gaussian) Ã‡ekirdeÄŸi: RBF, Gauss Ã§ekirdeÄŸi olarak da bilinir ve veriler arasÄ±ndaki mesafeleri kullanarak ayrÄ±m yapar.

ğŸ” RBF Kernelâ€™in Ã–zellikleri: ğŸ”

Yerel Ä°liÅŸkiler: RBF, yakÄ±n komÅŸulara daha fazla Ã¶nem verir. Yani, veriler arasÄ±ndaki mesafeleri ve yerel yapÄ±larÄ± Ã¶ÄŸrenir.

YÃ¼ksek Boyutlu Verilerde GÃ¼Ã§lÃ¼dÃ¼r: DÃ¼ÅŸÃ¼k boyutlu verilerde de etkili olabilir, ancak Ã¶zellikle yÃ¼ksek boyutlu veri kÃ¼melerinde daha gÃ¼Ã§lÃ¼dÃ¼r.

 âš ï¸ Gamma (Î³) Parametresi âš ï¸

Gamma (Î³), RBF kernel'inin etki alanÄ±nÄ± belirler. Gamma'nÄ±n deÄŸeri, Ã§ekirdeÄŸin ne kadar hassas veya geniÅŸ olacaÄŸÄ±nÄ± kontrol eder.

KÃ¼Ã§Ã¼k Gamma: Karar sÄ±nÄ±rlarÄ± daha dÃ¼z olur, model daha az hassaslaÅŸÄ±r (underfitting riski).
BÃ¼yÃ¼k Gamma: Karar sÄ±nÄ±rlarÄ± daha keskin olur, model daha hassas hale gelir (overfitting riski).

1. Gamma = 0.003 (KÃ¼Ã§Ã¼k Gamma) âš–ï¸

svm_model = SVC(kernel="rbf", gamma=0.003)
svm_model.fit(X,y)
plot_svm_boundary(svm_model, X,y)

Karar sÄ±nÄ±rÄ±: DÃ¼z ve genelleyici. ğŸŒ„
Model Tepkisi: DÃ¼ÅŸÃ¼k gamma deÄŸeri, modelin daha dÃ¼z tepki vermesine neden olur. Veriyi Ã§ok sert ÅŸekilde ayÄ±rmaz, daha geniÅŸ bir bÃ¶lgeyi kapsar.
Genelleme: Bu modelin genelleme kapasitesi yÃ¼ksek. ğŸ˜Œ EÄŸitim verisine aÅŸÄ±rÄ± uymadÄ±ÄŸÄ±ndan, yeni verilere de daha iyi tahminler yapma potansiyeli taÅŸÄ±r.
Risk: Bu durumda overfitting riski daha dÃ¼ÅŸÃ¼k, ancak bazÄ± kÃ¼Ã§Ã¼k yapÄ±larÄ± kaÃ§Ä±rma ihtimali de olabilir.
DÃ¼ÅŸÃ¼k gamma, modelin daha sade ve dÃ¼zgÃ¼n tepki vermesini saÄŸlar. Ancak Ã§ok kÃ¼Ã§Ã¼k gamma, bazÄ± karmaÅŸÄ±k iliÅŸkileri gÃ¶z ardÄ± edebilir.

2. Gamma = 0.3 (BÃ¼yÃ¼k Gamma) ğŸ”¥

svm_model = SVC(kernel="rbf", gamma=0.3)
svm_model.fit(X,y)
plot_svm_boundary(svm_model, X,y)

Karar sÄ±nÄ±rÄ±: Daha karmaÅŸÄ±k ve keskin. ğŸ’€
Model Tepkisi: Daha hassas ve ince ayrÄ±ntÄ±lara dikkat eden bir karar sÄ±nÄ±rÄ± oluÅŸturur. EÄŸitim verisine Ã§ok daha yakÄ±n tepki verir.
Overfitting: Bu deÄŸer, modelin eÄŸitildiÄŸi verilere fazla uyum saÄŸlamasÄ±na (overfitting) yol aÃ§ar. Ã‡Ã¼nkÃ¼ Ã§ok fazla ince ayrÄ±ntÄ±ya girer, genel yapÄ±dan sapabilir. ğŸ§ 
Risk: Overfitting nedeniyle, model yalnÄ±zca eÄŸitim verisine iyi uyum saÄŸlar, ama yeni verilere uygulandÄ±ÄŸÄ±nda performansÄ± dÃ¼ÅŸebilir. ğŸ“‰
YÃ¼ksek gamma, modelin Ã§ok hassas ve eÄŸitim verisine yakÄ±n tepki vermesini saÄŸlar. Ancak bu, genelleme kapasitesini sÄ±nÄ±rlayarak sadece eÄŸitim seti Ã¼zerinde baÅŸarÄ±lÄ± olmasÄ±na yol aÃ§ar.

3. VarsayÄ±lan Gamma (Sadece RBF) ğŸ”„

svc_model = SVC(kernel="rbf")
svc_model.fit(X,y)
plot_svm_boundary(svc_model , X, y)

Karar sÄ±nÄ±rÄ±: Orta seviye karmaÅŸÄ±klÄ±kta. âš™ï¸
Model Tepkisi: VarsayÄ±lan gamma, modelin dengeyi saÄŸladÄ±ÄŸÄ± bir durumdur. Ne Ã§ok dÃ¼z ne de Ã§ok hassastÄ±r.
Overfitting Riski: Bu model, gamma deÄŸeri Ã¼zerinden dÃ¼zgÃ¼n bir sÄ±nÄ±r belirler ve genellikle daha iyi genellemeler yapar. Ã‡Ã¼nkÃ¼ eÄŸitim verisine aÅŸÄ±rÄ± uyum saÄŸlamaz.
VarsayÄ±lan gamma, modelin eÄŸitim verisine ne Ã§ok uyum saÄŸlamasÄ±nÄ± ne de Ã§ok uzaklaÅŸmasÄ±nÄ± saÄŸlayan bir dengeyi kurar. Bu, genelleme kapasitesini optimize eder.

Ã–zetle:

KÃ¼Ã§Ã¼k gamma (0.003 gibi) daha dÃ¼z tepki verir ve genelleme yapma konusunda iyi sonuÃ§lar verir. ğŸŒ³
BÃ¼yÃ¼k gamma (0.3 gibi) aÅŸÄ±rÄ± hassas olup, overfitting riskine yol aÃ§ar, bu da modelin yalnÄ±zca eÄŸitim verisine uygun hale gelmesine neden olur. ğŸ›‘
VarsayÄ±lan gamma en dengeli yaklaÅŸÄ±mÄ± sunar ve genellikle iyi sonuÃ§lar elde edilir. âš–ï¸
Modelin gama deÄŸeri ne kadar bÃ¼yÃ¼k olursa, karar sÄ±nÄ±rlarÄ± o kadar karmaÅŸÄ±klaÅŸÄ±r, ancak bu genelleme kabiliyetini azaltÄ±r! ğŸ¯

ğŸ“ˆ RBF'nin Ã‡izimdeki Etkisi ğŸ“ˆ

Ã‡izgi: Lineer modeldeki dÃ¼z Ã§izgi yerini, eÄŸri bir karar sÄ±nÄ±rÄ±na bÄ±rakÄ±r.
AyrÄ±m: KarmaÅŸÄ±k verilerde doÄŸrusal olmayan ayrÄ±m yapÄ±lmasÄ±na imkan verir, bÃ¶ylece doÄŸrusal olmayan verileri de baÅŸarÄ±yla sÄ±nÄ±flandÄ±rabilir.
RBF Kernel, veriler arasÄ±nda daha esnek ve detaylÄ± bir ayrÄ±m yapmanÄ±zÄ± saÄŸlar, Ã¶zellikle karmaÅŸÄ±k ve doÄŸrusal olmayan veri kÃ¼melerinde gÃ¼Ã§lÃ¼ sonuÃ§lar elde edilir.


	ğŸ“Œ Sigmoid Kernel

Sigmoid kernel, SVM (Support Vector Machine) algoritmasÄ±nda doÄŸrusal olmayan ayrÄ±m gerektiren durumlar iÃ§in kullanÄ±lÄ±r ve sinir aÄŸlarÄ±ndaki aktivasyon fonksiyonu olan tanh (hiperbolik tanjant) fonksiyonuna benzer bir yaklaÅŸÄ±m gÃ¶sterir. Bu Ã¶zellik, SVM'yi bir tÃ¼r yapay sinir aÄŸÄ± gibi Ã§alÄ±ÅŸtÄ±rmanÄ±za olanak tanÄ±r. ğŸ¤–

ğŸ¤– Sigmoid Kernelâ€™in Ã–zellikleri:

1. Sinir AÄŸlarÄ±na Benzer DavranÄ±r ğŸ§ 

Sigmoid kernel, sinir aÄŸlarÄ±ndaki tanh fonksiyonunu kullanarak esnek ve karmaÅŸÄ±k karar sÄ±nÄ±rlarÄ± oluÅŸturur.
Model, veriler arasÄ±ndaki doÄŸrusal olmayan iliÅŸkileri Ã¶ÄŸrenir ve bunlarÄ± sÄ±nÄ±flandÄ±rÄ±r. SVM, sinir aÄŸlarÄ± gibi Ã§alÄ±ÅŸarak daha dinamik sonuÃ§lar elde eder! ğŸŒ

2. DoÄŸrusal Olmayan Problemleri Ã‡Ã¶zebilir ğŸ”„

Sigmoid kernel, Ã¶zellikle lojistik regresyon gibi doÄŸrusal olmayan karar sÄ±nÄ±rlarÄ± oluÅŸturur.
Ancak, RBF kernel kadar gÃ¼Ã§lÃ¼ deÄŸildir ve daha nadiren kullanÄ±lÄ±r. RBF kernel daha geliÅŸmiÅŸ ve gÃ¼Ã§lÃ¼ karar sÄ±nÄ±rlarÄ± oluÅŸturabilir. ğŸ˜

3. C ve Gamma (Î³) Parametrelerinden Etkilenir âš™ï¸

C parametresi: C bÃ¼yÃ¼kse, model daha katÄ± hale gelir ve overfitting riski artar. Yani, model eÄŸitim verisine fazla uyum saÄŸlar. âš ï¸
Gamma (Î³): Gamma bÃ¼yÃ¼kse, karar sÄ±nÄ±rlarÄ± daha keskin ve karmaÅŸÄ±k olur, model daha hassas hale gelir. ğŸ”¥

ğŸ•° Sigmoid Kernel Ne Zaman KullanÄ±lÄ±r? ğŸ•°

1. Sinir AÄŸlarÄ± Benzeri Bir YaklaÅŸÄ±m Ä°steniyorsa ğŸ§ 

EÄŸer SVM modelinin sinir aÄŸÄ± gibi Ã§alÄ±ÅŸmasÄ±nÄ± istiyorsan, sigmoid kernel en iyi tercihtir! Bu kernel, daha esnek bir yapÄ± sunar. ğŸ¤¹â€â™€ï¸
2. Veri DoÄŸrusal Olarak AyrÄ±lamÄ±yorsa Ancak DoÄŸrusal Modele YakÄ±nsa ğŸ“‰

EÄŸer veriler tam doÄŸrusal olarak ayrÄ±lamÄ±yorsa ama doÄŸrusal sÄ±nÄ±rlara yakÄ±n bir yapÄ±daysa, sigmoid kernel doÄŸrusal olmayan ancak yakÄ±n sÄ±nÄ±flandÄ±rmalar yapmak iÃ§in uygun olur! âš–ï¸
3. Veriler ArasÄ±nda Sigmoid Benzeri Bir Ä°liÅŸki Varsa ğŸ”„

Veriler arasÄ±nda sigmoid benzeri  bir iliÅŸki varsa, sigmoid kernel kullanÄ±larak bu tÃ¼r iliÅŸkilere uygun karar sÄ±nÄ±rlarÄ± oluÅŸturulabilir! ğŸ”€

Ã–rnek Kod (Sigmoid Kernel ile SVM):

svc_model = SVC(kernel="sigmoid")
svc_model.fit(X, y)
plot_svm_boundary(svc_model, X, y)

Bu kod parÃ§asÄ±, sigmoid kernel kullanan bir SVM modelini oluÅŸturur. EÄŸitim verileri Ã¼zerinde karar sÄ±nÄ±rlarÄ±nÄ± Ã§izmek iÃ§in plot_svm_boundary fonksiyonu kullanÄ±lÄ±r.

âš–ï¸ Sigmoid Kernelâ€™in Ã‡izimdeki Etkisi: âš–ï¸

Karar SÄ±nÄ±rÄ±: Sigmoid kernel, doÄŸrusal olmayan karar sÄ±nÄ±rlarÄ± oluÅŸturur. Bu sÄ±nÄ±r S ÅŸeklinde olabilir ve doÄŸrusal olmayan verileri baÅŸarÄ±yla ayÄ±rabilir. ğŸ”

AyrÄ±m: Sigmoid kernel, veriler arasÄ±ndaki doÄŸrusal olmayan iliÅŸkileri daha iyi modelleyebilir. Bu sayede, karmaÅŸÄ±k veri kÃ¼melerinde esnek ve doÄŸru ayrÄ±mlar yapÄ±labilir. ğŸŒ

Ã–zetle:

Sigmoid kernel, doÄŸrusal olmayan ayrÄ±m gerektiren durumlarda kullanÄ±lÄ±r ve sinir aÄŸlarÄ± gibi Ã§alÄ±ÅŸarak esnek karar sÄ±nÄ±rlarÄ± oluÅŸturur.
C ve gamma parametreleri, modelin esnekliÄŸini ve karar sÄ±nÄ±rlarÄ±nÄ±n karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± etkiler.
Sigmoid kernel, Ã¶zellikle sinir aÄŸÄ± benzeri bir yapÄ± isteniyorsa, veriler doÄŸrusal olarak ayrÄ±lamÄ±yorsa ve sigmoid benzeri iliÅŸkiler varsa kullanÄ±labilir. ğŸš€

ğŸŒ 

# Sigmoid Kernel ile SVM Modeli (C ve Gamma parametreleri ile)
svc_model = SVC(kernel="sigmoid", C=1.0, gamma=0.5)  # Burada C ve gamma deÄŸerlerini belirliyoruz
svc_model.fit(X, y)

ğŸŒ


âšª C Parametresi: âšª

C deÄŸeri, modelin ne kadar katÄ± veya esnek olacaÄŸÄ±nÄ± belirler.
C bÃ¼yÃ¼kse, model daha katÄ± olur ve overfitting riski artar. Yani, model eÄŸitim verisine daha fazla uyum saÄŸlar. ğŸ”¥
C kÃ¼Ã§Ã¼kse, model daha genel ve esnek olur, ama underfitting riski de artabilir. ğŸŒ³

âšª Gamma Parametresi (Î³): âšª

Gamma, kernel'in etki alanÄ±nÄ± belirler. Yani, gamma deÄŸeri ne kadar bÃ¼yÃ¼kse, modelin karar sÄ±nÄ±rlarÄ± o kadar keskin ve karmaÅŸÄ±k olur. Bu da overfitting riskini artÄ±rÄ±r. ğŸ”¥
Gamma kÃ¼Ã§Ã¼kse, karar sÄ±nÄ±rlarÄ± daha dÃ¼z olur ve model daha genel bir yaklaÅŸÄ±m sergiler, bu da underfitting riski yaratabilir. ğŸŒ„

âšª Sigmoid Kernel'de C ve Gamma'nÄ±n RolÃ¼: âšª

C parametresi, sigmoid kernel'in karar sÄ±nÄ±rlarÄ±nÄ±n katÄ±lÄ±ÄŸÄ±nÄ± kontrol eder.
Gamma parametresi ise sigmoid kernel'in karar sÄ±nÄ±rlarÄ±nÄ±n keskinliÄŸini ve karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± belirler.

Ã–zetle:

Sigmoid kernel de C ve gamma parametrelerini alÄ±r ve bu parametreler, modelin nasÄ±l davranacaÄŸÄ±nÄ± etkiler. C ve gamma deÄŸerlerini doÄŸru seÃ§mek, modelin performansÄ± ve overfitting/underfitting dengesini kurmak aÃ§Ä±sÄ±ndan kritik rol oynar.


	ğŸ“Œ Poly Kernel (Polinom Kernel) ğŸ“Œ

Polinom kernel, SVM (Support Vector Machine) algoritmasÄ±nda, doÄŸrusal olmayan veri kÃ¼melerini sÄ±nÄ±flandÄ±rmak iÃ§in kullanÄ±lan bir kernel tÃ¼rÃ¼dÃ¼r. Bu kernel, polinom fonksiyonlarÄ± kullanarak, doÄŸrusal olarak ayrÄ±lamayan verilerin doÄŸrusal olmayan ayrÄ±mlarÄ±nÄ± yapmanÄ±za olanak tanÄ±r.

ğŸ§‘â€ğŸ’» Poly Kernelâ€™in Ã–zellikleri: ğŸ§‘â€ğŸ’»

Polinom FonksiyonlarÄ± KullanÄ±r:

Poly kernel, polinom fonksiyonlarÄ± kullanarak verileri sÄ±nÄ±flandÄ±rÄ±r. Bu sayede doÄŸrusal olmayan sÄ±nÄ±flandÄ±rmalarda doÄŸrusal olmayan karar sÄ±nÄ±rlarÄ± oluÅŸturulabilir.
 
Burada x ve y, iki veri Ã¶rneÄŸi, c bir sabit terim, d ise polinomun derecesidir.
Derece (Degree) Parametresi:

Degree (derece) parametresi, polinom kernel'inin derecesini belirler. Degree bÃ¼yÃ¼dÃ¼kÃ§e, karar sÄ±nÄ±rlarÄ± daha karmaÅŸÄ±k ve esnek olur.

Ã–rneÄŸin, degree=2 seÃ§ildiÄŸinde, model ikinci dereceden bir polinom kullanarak karar sÄ±nÄ±rlarÄ±nÄ± oluÅŸturur. Bu, veri setindeki daha karmaÅŸÄ±k iliÅŸkileri Ã¶ÄŸrenebilmesine olanak saÄŸlar.

âšª C ve Gamma Parametrelerinden Etkilenir: âšª

C: C parametresi, modelin katÄ±lÄ±ÄŸÄ±nÄ± belirler. YÃ¼ksek C deÄŸeri, modelin eÄŸitim verilerine daha fazla uyum saÄŸlamasÄ±na neden olur ve overfitting riski oluÅŸturabilir.
Gamma (Î³): Gamma deÄŸeri, kernelâ€™in etki alanÄ±nÄ± belirler. KÃ¼Ã§Ã¼k gamma daha dÃ¼z karar sÄ±nÄ±rlarÄ± oluÅŸturur, bÃ¼yÃ¼k gamma ise daha keskin ve hassas karar sÄ±nÄ±rlarÄ± oluÅŸturur.

ğŸ“Š Poly Kernel Ã–rneÄŸi: ğŸ“Š

AÅŸaÄŸÄ±daki Ã¶rnekte, polynomial kernel kullanarak bir SVM modeli oluÅŸturuyoruz ve modelin karar sÄ±nÄ±rlarÄ±nÄ± Ã§iziyoruz:

# Polynomial Kernel ile SVM Modeli (degree parametresi ile)
svm_model = SVC(kernel="poly", degree=2, C=1.0, gamma='scale')  # degree, C ve gamma parametrelerini belirliyoruz
svm_model.fit(X, y)

# EÄŸitim verileri Ã¼zerinde karar sÄ±nÄ±rlarÄ±nÄ± Ã§iziyoruz
plot_svm_boundary(svm_model, X, y)

ğŸ” Poly Kernelâ€™in Etkisi:

Degree=2: Ä°kinci dereceden bir polinom kullanarak karar sÄ±nÄ±rlarÄ±nÄ± oluÅŸturur. Bu, modelin verileri daha karmaÅŸÄ±k bir ÅŸekilde ayÄ±rmasÄ±nÄ± saÄŸlar. Ancak, degree deÄŸeri arttÄ±kÃ§a model daha karmaÅŸÄ±k hale gelir ve overfitting riski artabilir.

C ve Gamma: C ve gamma parametreleri, modelin esnekliÄŸini ve karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± kontrol eder.

ğŸ Ne Zaman KullanÄ±lÄ±r? ğŸ

DoÄŸrusal Olmayan SÄ±nÄ±flandÄ±rmalar: Veri setinde doÄŸrusal olmayan iliÅŸkiler varsa, polynomial kernel Ã§ok iyi Ã§alÄ±ÅŸabilir.

KarmaÅŸÄ±k Karar SÄ±nÄ±rlarÄ± Gerektiren Durumlar: EÄŸer veri setindeki sÄ±nÄ±flar birbirinden doÄŸrusal olarak ayrÄ±lamÄ±yorsa ve daha karmaÅŸÄ±k bir karar sÄ±nÄ±rÄ± oluÅŸturmak istiyorsanÄ±z, polinom kernel kullanmak faydalÄ±dÄ±r.

ğŸ“‰ Poly Kernelâ€™in Riskleri: ğŸ“‰

Overfitting Riski: Derece arttÄ±kÃ§a, model daha esnek hale gelir ve overfitting riski artar. Model, eÄŸitim verisine Ã§ok fazla uyum saÄŸlayarak yeni verilere genelleme yapamayabilir.


				ğŸ“Œ 5. Grid Search ile En Ä°yi Modeli Bulma

from sklearn.model_selection import GridSearchCV

svm = SVC()

C = 10.0 ** np.arange(-4,3)# 10^-4 - 10^2

# BaÅŸlangÄ±cÄ± alÄ±r sondakinin 1 eksiÄŸine kadar gider
# Hata almamak iÃ§in 10./10.0  tarzÄ±nda yazmalÄ±sÄ±n


parameters={
    "C":C,
    "kernel":["rbf","linear"]
    }
    
grid = GridSearchCV(svm,parameters )
grid.fit(X,y)
grid.best_score_
grid.best_params_


ğŸ“Œ Grid Search Ä°ÅŸlemi AÃ§Ä±klamasÄ±:

ğŸ“Š C Parametresi iÃ§in AralÄ±k Belirleme: ğŸ“Š

C = 10.0 ** np.arange(-4, 3)  # 10^-4 ile 10^2 arasÄ±ndaki deÄŸerler

C parametresi, modelin esnekliÄŸini belirler. KÃ¼Ã§Ã¼k C deÄŸerleri modelin genelleme yapmasÄ±nÄ± saÄŸlar, bÃ¼yÃ¼k C deÄŸerleri ise modelin eÄŸitim verisine Ã§ok fazla uymasÄ±na neden olabilir (overfitting). ğŸ˜¬
Burada C iÃ§in farklÄ± deÄŸerler oluÅŸturuluyor: 
10^-4, 10^-3, 10^-2, 10^-1, 1, 10, 100, 1000. ğŸš€

ğŸ“š Parametreler SÃ¶zlÃ¼ÄŸÃ¼: ğŸ“š

parameters = {
    "C": C,
    "kernel": ["rbf", "linear"]
}

C parametresi yukarÄ±da oluÅŸturduÄŸumuz deÄŸerlerle tanÄ±mlanÄ±yor. ğŸ§‘â€ğŸ’»
kernel parametresi, SVM'in hangi Ã§ekirdek fonksiyonlarÄ±nÄ± kullanacaÄŸÄ±nÄ± belirliyor. Burada "rbf" (Radial Basis Function) ve "linear" (doÄŸrusal) kernelâ€™ler deneniyor. ğŸ§ 

âš™ï¸ GridSearchCV Modelini TanÄ±mlama ve EÄŸitme: âš™ï¸

grid = GridSearchCV(svm, parameters)
grid.fit(X, y)

GridSearchCV burada, belirtilen parametre kombinasyonlarÄ± Ã¼zerinde modelin eÄŸitim sÃ¼recini baÅŸlatÄ±yor. ğŸ”„

Her bir C ve kernel kombinasyonu denendikten sonra, en iyi sonuÃ§lar (yani en iyi performans) elde ediliyor. ğŸ”¥

ğŸ† En Ä°yi SonuÃ§larÄ± Alma: ğŸ†

grid.best_score_
grid.best_params_

grid.best_score_: Bu, en iyi parametre kombinasyonu ile elde edilen en yÃ¼ksek doÄŸruluk deÄŸerini dÃ¶ndÃ¼rÃ¼yor. ğŸ¯
grid.best_params_: En iyi sonucu veren C ve kernel kombinasyonlarÄ±nÄ± dÃ¶ndÃ¼rÃ¼yor. ğŸ‰

Ã–zet: ğŸ“ˆ

GridSearchCV ile, modelin hiperparametrelerini test ediyoruz: C ve kernel deÄŸerleri iÃ§in tÃ¼m kombinasyonlarÄ± deniyoruz. ğŸ”
Modelin hangi parametrelerle en iyi performansÄ± gÃ¶sterdiÄŸini buluyoruz. ğŸš€
Bu sÃ¼reÃ§, modelin daha iyi genelleme yapabilmesi iÃ§in oldukÃ§a Ã¶nemli! ğŸ’¡
Hedefimiz: Modelin performansÄ±nÄ± optimize etmek ve en iyi parametreleri bulmak! âœ¨


				ğŸ“Œ 7. Final Model ve DeÄŸerlendirme

GridSearch ile yapÄ±lan iÅŸlemlerin sonucunda aÅŸaÄŸÄ±da olduÄŸu gibi bir deÄŸer skalasÄ± Ã§Ä±kmÄ±ÅŸtÄ±r.

ğŸš€{'C': np.float64(0.0001), 'kernel': 'rbf'} ğŸš€

Bunu final modeline uyarlayalÄ±m

svm = SVC(kernel="rbf" , C=0.0001)
svm.fit(X,y)
plot_svm_boundary(svm_model, X,y)
svm_pred =svm.predict(X)

from sklearn.metrics import ConfusionMatrixDisplay ,classification_report

ConfusionMatrixDisplay.from_estimator(svm,X,y)
print(classification_report(y,svm_pred))

* KullanÄ±lacak kernellar ve C deÄŸerlerinin etkileri yukarÄ±da gÃ¶zÃ¼ktÃ¼ÄŸÃ¼ gibidir.

* GridSearch kullanarak yapÄ±lan iÅŸlemlerde hangisinin en iyi sonuÃ§ verileceÄŸi bulunmuÅŸ ve denenmiÅŸtir.

* Ä°sterseniz yukarÄ±da kendimizin denediÄŸi deÄŸerler ile gÃ¶rselleÅŸtirme yaparak daha detaylÄ± farklarÄ± gÃ¶rebilirsiniz


  ğŸ“ŠğŸ” GridSearchCV sonucunda elde edilen en iyi parametreleri ve skorlarÄ± tablo ve grafik olarak gÃ¶steren bir fonksiyonu aÃ§Ä±klayalÄ±m. ğŸ“ŠğŸ”


				ğŸ“Œ Grid_Plot.py ğŸ“Œ


ğŸ“Œ 1. Fonksiyonun AmacÄ±

def GridSearch_table_plot(grid_clf, param_name, num_results=15, negative=True, graph=True, display_all_params=True):

GridSearchCV'nin sonuÃ§larÄ±nÄ± daha iyi anlamak iÃ§in tablolar ve grafikler oluÅŸturan bir yardÄ±mcÄ± fonksiyon.

param_name: Hangi hiperparametrenin incelendiÄŸini belirtiyor. ğŸ”¢

num_results: KaÃ§ sonucu gÃ¶stereceÄŸimizi belirtiyor. ğŸ“œ

negative: Skor negatifse ters Ã§eviriyor (Ã¶rneÄŸin neg_log_loss gibi metriklerde). ğŸ”„
graph: Grafik Ã§izilsin mi? ğŸ¨

display_all_params: TÃ¼m parametreleri gÃ¶sterelim mi? ğŸ§

ğŸ“Œ 2. En Ä°yi Parametreleri ve Skoru Bulma

clf = grid_clf.best_estimator_
clf_params = grid_clf.best_params_
if negative:
    clf_score = -grid_clf.best_score_
else:
    clf_score = grid_clf.best_score_
clf_stdev = grid_clf.cv_results_['std_test_score'][grid_clf.best_index_]
cv_results = grid_clf.cv_results_
En iyi modeli (best_estimator_) alÄ±yoruz. ğŸ†
En iyi hiperparametreleri (best_params_) Ã§Ä±karÄ±yoruz. ğŸ› ï¸
En iyi skoru alÄ±p negatifse ters Ã§eviriyoruz. ğŸ”¢
Standart sapmayÄ± (std_test_score) alÄ±yoruz. ğŸ“Š

ğŸ“Œ 3. SonuÃ§larÄ± YazdÄ±rma

print("best parameters: {}".format(clf_params))
print("best score:      {:0.5f} (+/-{:0.5f})".format(clf_score, clf_stdev))
if display_all_params:
    import pprint
    pprint.pprint(clf.get_params())
En iyi parametreleri ve skoru ekrana yazdÄ±rÄ±yoruz. ğŸ–¥ï¸
TÃ¼m parametreleri gÃ¶rmek istiyorsak pprint.pprint() ile bastÄ±rÄ±yoruz. ğŸ“„

ğŸ“Œ 4. En Ä°yi SonuÃ§larÄ± SeÃ§me

scores_df = pd.DataFrame(cv_results).sort_values(by='rank_test_score')
best_row = scores_df.iloc[0, :]
if negative:
    best_mean = -best_row['mean_test_score']
else:
    best_mean = best_row['mean_test_score']
best_stdev = best_row['std_test_score']
best_param = best_row['param_' + param_name]
SonuÃ§larÄ± rank_test_score'a gÃ¶re sÄ±ralÄ±yoruz (en iyi sonuÃ§ en Ã¼stte olacak). ğŸ“ˆ
En iyi sonucu (best_row) alÄ±yoruz. ğŸ¥‡
En iyi skorun ortalama ve standart sapmasÄ±nÄ± buluyoruz. ğŸ“Š
Ä°lgili parametrenin en iyi deÄŸerini (best_param) alÄ±yoruz. ğŸ¯

ğŸ“Œ 5. Ä°lk num_results Kadar Sonucu GÃ¶sterme

display(pd.DataFrame(cv_results).sort_values(by='rank_test_score').head(num_results))
En iyi num_results kadar sonucu tablo halinde gÃ¶steriyoruz. ğŸ†

ğŸ“Œ 6. SonuÃ§larÄ± Grafikte GÃ¶sterme

scores_df = scores_df.sort_values(by='param_' + param_name)
if negative:
    means = -scores_df['mean_test_score']
else:
    means = scores_df['mean_test_score']
stds = scores_df['std_test_score']
params = scores_df['param_' + param_name]
Ä°lgili hiperparametreye gÃ¶re sÄ±ralÄ±yoruz. ğŸ“
SkorlarÄ± alÄ±p negatifse ters Ã§eviriyoruz. ğŸ”„
Standart sapma ve hiperparametre deÄŸerlerini Ã§Ä±karÄ±yoruz. ğŸ“Š

ğŸ“Œ 7. GrafiÄŸi Ã‡izme

if graph:
    plt.figure(figsize=(8, 8))
    plt.errorbar(params, means, yerr=stds)

    plt.axhline(y=best_mean + best_stdev, color='red')
    plt.axhline(y=best_mean - best_stdev, color='red')
    plt.plot(best_param, best_mean, 'or')

    plt.title(param_name + " vs Score\nBest Score {:0.5f}".format(clf_score))
    plt.xlabel(param_name)
    plt.ylabel('Score')

    plt.xscale('log')

    plt.show()

EÄŸer graph=True ise grafiÄŸi Ã§iziyoruz. ğŸ¨

X eksenine hiperparametre deÄŸerlerini, Y eksenine skoru koyuyoruz. ğŸ“ˆ
Hata Ã§ubuklarÄ± (errorbar) ile standart sapmayÄ± gÃ¶steriyoruz. ğŸ“

En iyi skoru kÄ±rmÄ±zÄ± noktalar ve Ã§izgilerle vurguluyoruz. ğŸš€

X eksenini logaritmik Ã¶lÃ§eÄŸe Ã§eviriyoruz (plt.xscale('log')) Ã§Ã¼nkÃ¼ hiperparametreler genellikle Ã¼stel olarak deÄŸiÅŸir. ğŸ”¢

ğŸ“Œ Ã–zet

Bu fonksiyon GridSearchCV'nin sonuÃ§larÄ±nÄ± daha iyi analiz etmemizi saÄŸlar:
âœ… En iyi hiperparametreleri ve skoru bulur ğŸ¯
âœ… SonuÃ§larÄ± tablo olarak gÃ¶sterir ğŸ“Š
âœ… Grafik ile performans deÄŸiÅŸimini gÃ¶rselleÅŸtirir ğŸ“ˆ

Bu fonksiyon hiperparametre optimizasyonunun nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± anlamak iÃ§in mÃ¼kemmel bir araÃ§tÄ±r! ğŸš€



				ğŸ“Œ Svm_margin_plotpy ğŸ“Œ

Bu fonksiyon, SVM (Support Vector Machine) modelinin karar sÄ±nÄ±rlarÄ±nÄ± ve destek vektÃ¶rlerini gÃ¶rselleÅŸtirir. ğŸ“ŠğŸ¤–

ğŸ“Œ 1. Fonksiyonun AmacÄ±

def plot_svm_boundary(model, X, y):

EÄŸitilmiÅŸ bir SVM modelinin karar sÄ±nÄ±rlarÄ±nÄ± Ã§izmek iÃ§in kullanÄ±lÄ±r.
Destek vektÃ¶rlerini vurgular.
Ä°ki boyutlu verilerle Ã§alÄ±ÅŸÄ±r, dolayÄ±sÄ±yla sadece iki Ã¶zelliÄŸi (X[:,0] ve X[:,1]) olan verilerde kullanÄ±labilir.

ğŸ“Œ 2. Veriyi HazÄ±rlama

X = X.values
y = y.values

Pandas DataFrame yerine NumPy dizisi olarak iÅŸlem yapabilmek iÃ§in .values kullanÄ±yoruz. ğŸ”„

ğŸ“Œ 3. Veri NoktalarÄ±nÄ± GrafiÄŸe DÃ¶kmek

plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='rainbow')
EÄŸitim verilerini bir scatter plot ile gÃ¶steriyoruz. ğŸ“
Renkler (c=y) sÄ±nÄ±flara gÃ¶re belirleniyor. ğŸŒˆ

ğŸ“Œ 4. Grafik Ä°Ã§in SÄ±nÄ±rlarÄ± Belirleme

ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()
Mevcut eksen (ax = plt.gca()) Ã¼zerinden X ve Y sÄ±nÄ±rlarÄ±nÄ± alÄ±yoruz. ğŸ”³

ğŸ“Œ 5. Modelin Karar SÄ±nÄ±rÄ±nÄ± Ã‡izmek Ä°Ã§in Izgara OluÅŸturma

xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T

X ve Y eksenlerinde 30 noktadan oluÅŸan bir grid (Ä±zgara) oluÅŸturuyoruz. ğŸ—ï¸
Bu gridin her noktasÄ± iÃ§in modelin karar fonksiyonunu hesaplayacaÄŸÄ±z.

ğŸ“Œ 6. Karar Fonksiyonunu Kullanarak SÄ±nÄ±rlarÄ± Belirleme
python
Kopyala
DÃ¼zenle
Z = model.decision_function(xy).reshape(XX.shape)
SVM'nin karar fonksiyonunu (decision_function) kullanarak grid noktalarÄ±nÄ±n hangi sÄ±nÄ±fa ait olduÄŸunu hesaplÄ±yoruz.
SonuÃ§larÄ± (Z) tekrar XX.shape boyutuna getiriyoruz.

ğŸ“Œ 7. Karar SÄ±nÄ±rlarÄ±nÄ± Ã‡izme

ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,
           linestyles=['--', '-', '--'])

contour() fonksiyonu ile karar sÄ±nÄ±rlarÄ±nÄ± Ã§iziyoruz.
Seviyeler (levels=[-1, 0, 1]):
0 Ã§izgisi karar sÄ±nÄ±rÄ±nÄ± gÃ¶sterir. âš–ï¸
-1 ve 1 Ã§izgileri margin Ã§izgileridir. ğŸš§

ğŸ“Œ 8. Destek VektÃ¶rlerini GÃ¶sterme

ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100,
           linewidth=1, facecolors='none', edgecolors='k')

SVM'nin belirlediÄŸi destek vektÃ¶rlerini Ã§ember iÃ§ine alÄ±yoruz. â­•
Destek vektÃ¶rleri modelin karar sÄ±nÄ±rÄ±nÄ± belirlemede en etkili olan noktalardÄ±r. ğŸ”‘

ğŸ“Œ Ã–zet
Bu fonksiyon, SVM modelinin nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± anlamak iÃ§in Ã§ok kullanÄ±ÅŸlÄ±dÄ±r:
âœ… Karar sÄ±nÄ±rÄ±nÄ± Ã§izer â¡ï¸ ğŸ“
âœ… Margin Ã§izgilerini gÃ¶sterir â¡ï¸ ğŸš§
âœ… Destek vektÃ¶rlerini vurgular â¡ï¸ â­•
âœ… SÄ±nÄ±flarÄ± renklerle ayÄ±rÄ±r â¡ï¸ ğŸŒˆ

SVM modelinizi eÄŸittikten sonra bu fonksiyon ile gÃ¶rselleÅŸtirme yaparak karar sÄ±nÄ±rlarÄ±nÄ± analiz edebilirsiniz! ğŸš€













